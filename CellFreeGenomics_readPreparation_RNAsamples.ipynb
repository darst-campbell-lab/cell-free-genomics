{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266b1e95",
   "metadata": {},
   "source": [
    "## Cell-Free Genomics: Read preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943305e",
   "metadata": {},
   "source": [
    "#### Ruby Froom, Campbell/Darst and Rock labs at Rockefeller University\n",
    "\n",
    "The following pipeline processes raw FASTQ files, maps reads and extracts R2 reads only (the desired ends of interest for either 5' or 3' end libraries)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e08aac1",
   "metadata": {},
   "source": [
    "## Modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c972395f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex\n",
    "import csv\n",
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "from Bio import SeqRecord\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f6dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by Peter Culviner, PhD to enable command-line access through Jupyter\n",
    "def quickshell(command, print_output=True, output_path=None, return_output=False):\n",
    "    process_output = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout = process_output.stdout.decode('utf-8')\n",
    "    stderr = process_output.stderr.decode('utf-8')\n",
    "    output_string = f'STDOUT:\\n{stdout}\\nSTDERR:\\n{stderr}\\n'\n",
    "    if print_output:\n",
    "        print('$ ' + command)\n",
    "        print(output_string)\n",
    "    if output_path is not None:\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(output_string)\n",
    "    if return_output:\n",
    "        return stdout, stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a6d32",
   "metadata": {},
   "source": [
    "## Initializing inputs and settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f713ddb9",
   "metadata": {},
   "source": [
    "Inputs for directory creation and pointing to files. \n",
    "\n",
    "The starting requirements for this pipeline are:\n",
    ">**main_path**: the absolute path of the working directory. Update to reflect your own configuration.\n",
    "\n",
    ">**input_csv_dir:** a directory containing `i7_sample_file` and `inline_sample_file` csv files (see below for file specifications) called `input_csv_files`.\n",
    "\n",
    ">**genome_dir:** a directory containing reference genome(s) for read mapping called `genomes`.\n",
    "\n",
    ">**raw_fastq_dir:** a directory containing your i7-demultiplexed fastq files called `raw_fastq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2b3db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing locations of input .csv files, alignment genomes and raw compressed fastq files\n",
    "main_path = '5enrich_CRP'\n",
    "#main_path = '3enrich_NusAG'\n",
    "\n",
    "input_csv_dir = f'{main_path}/input_csv_files'\n",
    "\n",
    "readPrep_dir = f'{main_path}/readPrep'\n",
    "genome_dir = 'genome_files_misc'\n",
    "raw_fastq_dir = f'{readPrep_dir}/raw_fastq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b238cf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "i7_table = pd.read_csv(f'{input_csv_dir}/i7_barcodes_5enrich.csv')\n",
    "inline_table = pd.read_csv(f'{input_csv_dir}/inline_barcodes_5enrich.csv')\n",
    "\n",
    "# i7_table = pd.read_csv(f'{input_csv_dir}/i7_barcodes_3enrich.csv')\n",
    "# inline_table = pd.read_csv(f'{input_csv_dir}/inline_barcodes_3enrich.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b219c5",
   "metadata": {},
   "source": [
    "The additional directories below are a suggested organization for subsequent processing steps in this notebook.\n",
    "\n",
    "The cells below will initialize these variables and create the directories inside the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09f6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastqc_dir = f'{readPrep_dir}/fastqc'\n",
    "\n",
    "# Directory for fastqs split by inline barcodes\n",
    "demultiplexed_fastq_dir = f'{readPrep_dir}/demultiplexed_fastq'\n",
    "\n",
    "# Directory for quality filtered and Illumina adaptor-trimmed fastq files\n",
    "trimmed_fastq_dir = f'{readPrep_dir}/trimmed_fastq'\n",
    "\n",
    "# Directory for containing converted fastas with UMIs removed for mapping\n",
    "noUMI_dir = f'{readPrep_dir}/UMIextract_fasta'\n",
    "combined_fasta_dir = f'{noUMI_dir}/combined_fastas'\n",
    "\n",
    "# Pre-processing alignments prior to enriched end calling\n",
    "initial_alignments_dir = f'{readPrep_dir}/initial_alignments'\n",
    "dedup_alignments_dir = f'{readPrep_dir}/dedup_alignments'\n",
    "dedup_logs_dir = f'{dedup_alignments_dir}/dedup_logs'\n",
    "R2_alignments_dir = f'{readPrep_dir}/R2_alignments'\n",
    "spike_R2_alignments_dir = f'{readPrep_dir}/spike_R2_alignments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6a59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $fastqc_dir\n",
    "!mkdir $demultiplexed_fastq_dir\n",
    "!mkdir $trimmed_fastq_dir\n",
    "!mkdir $noUMI_dir\n",
    "!mkdir $combined_fasta_dir\n",
    "!mkdir $initial_alignments_dir\n",
    "!mkdir $dedup_alignments_dir\n",
    "!mkdir $dedup_logs_dir\n",
    "!mkdir $R2_alignments_dir\n",
    "!mkdir $spike_R2_alignments_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0effe907",
   "metadata": {},
   "source": [
    "*Descriptions written by Peter Culviner, PhD with modifications from Ruby Froom.*\n",
    "\n",
    "**threads:** number of threads to use in command-line calls (e.g. fastqc, cutadapt, bwa, end calling).\n",
    "\n",
    "**inline_barcode_errors:** How many mismatches can be tolerated in the inline barcode. Analyze the edit distance between the barcodes used to assess a tolerable number of errors.\n",
    "\n",
    "**barcode_length**: the length of the inline barcodes. Enables calculation of the error rate (# errors / barcode length), a required input for `cutadapt` when we are splitting based on inline barcode identification.\n",
    "\n",
    "**minimum_insert_length:** minimum allowed length of insert after trimming of adapter sequences at the 3'-ends of read1 and read2.\n",
    "\n",
    "**i7_trim**: For 3'-end trimming of the first read. The adapter sequence to look for at the 3'-end of the read. Note that the reverse complement of the inline barcode for a given sample will be appended to the front of this string since the inline barcode would appear just before this string.\n",
    "\n",
    "**i5_trim**: For 3'-end trimming of the second read. The adapter sequence to look for at the 3'-end of the read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eec0d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTINGS\n",
    "# Threads (CPU x2 is max) for fastqc, cutadapt, bwa, and end calling\n",
    "threads = 18\n",
    "\n",
    "# inline adapter identification\n",
    "inline_barcode_errors = 1\n",
    "barcode_length = 8\n",
    "inline_barcode_error_rate = inline_barcode_errors / barcode_length\n",
    "\n",
    "# Quality filters\n",
    "minimum_insert_length = 10\n",
    "quality_threshold = 20\n",
    "\n",
    "# 3'-adapter trimming\n",
    "i7_trim = 'GATCGGAAGAGCACACGTCTGAACTCCAGTCAC'\n",
    "i5_trim = 'GATCGTCGGACTGTAGAACTCTGAACGTGTAGATCTCGGTGGTCGCCGTATCATT'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6301f224",
   "metadata": {},
   "source": [
    "*Descriptions written by Peter Culviner, PhD with modifications from Ruby Froom.*\n",
    "\n",
    "**i7_sample_file**: Used to identify fastq files to split from. Assumes we're working with fastqs that have already been split by Illumina's i7 indexes. Minimally should have columns:\n",
    ">**r1**: read 1 fastq, no inline barcode, template starts from first base.\n",
    "\n",
    ">**r2**: read 2 fastq, this one is assumed to have the inline barcode starting from the first base.\n",
    "\n",
    ">**i7**: number to designate this fastq, used internally by the code and used to designate in which unsplit fastq files final samples exist. minimally, each fastq pair should have a unique number.\n",
    "\n",
    ">**title**: used internally for pool names during splitting, unique but the name itself is not important.\n",
    "\n",
    "**inline_sample_file**: Used to determine which inline barcodes should exist in each fastq file in the i7 sample file (i.e., not all i7 sample files need to have the same inline barcodes). Minimally should have columns:\n",
    ">**sequence**: sequence of the barcode as it is read 5' -> 3' on the RT primer. This sequence will be searched for by cutadapt at the beginning of each read 2 to split then its reverse complement will be used to identify if the 3'-ends of read 1 need to be trimmed (i.e., if the insert was completely read through and we started sequencing the adapter sequence).\n",
    "\n",
    ">**inline_barcode:** number used to designate this inline barcode, used internally by the code. minimally, each barcode sequence should have a unique number.\n",
    "\n",
    ">**title**: final intended title for this sample. Split fastqs will be titled **title**.R1/R2.fastq.gz\n",
    "\n",
    ">**in_i7**: which i7 number (see above) to search for this sample in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeddf29f",
   "metadata": {},
   "source": [
    "## Generate quality reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17d431a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in i7_table.iterrows():\n",
    "    _, i7_data = row\n",
    "    R1 = i7_data.r1\n",
    "    R2 = i7_data.r2\n",
    "\n",
    "    command1 = f'fastqc {raw_fastq_dir}/{R1} -o {fastqc_dir} -t {threads}'\n",
    "    command2 = f'fastqc {raw_fastq_dir}/{R2} -o {fastqc_dir} -t {threads}'\n",
    "\n",
    "    quickshell(\n",
    "            command1,\n",
    "            print_output=False,\n",
    "            return_output=False)\n",
    "\n",
    "    print(f'R1 fastqc: {i7_data.title} done')\n",
    "    \n",
    "    quickshell(\n",
    "            command2,\n",
    "            print_output=False,\n",
    "            return_output=False)\n",
    "    print(f'R2 fastqc: {i7_data.title} done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f053e",
   "metadata": {},
   "source": [
    "## Split reads based on inline barcodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe20f05",
   "metadata": {},
   "source": [
    "The inline barcodes were optimized, and the parsing code was written, by Peter Culviner, PhD (Fortune Lab, Harvard).\n",
    "\n",
    "Each i7-demultiplexed fastq file is further split based upon the presence of inline barcodes. Reads are then quality- and length-filtered, and adaptors (sequences assigned to `i7_trim` and `i5_trim`) are trimmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a9e6b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demultiplexing code\n",
    "# Written by Peter Culviner, PhD\n",
    "\n",
    "# list of reports for storage\n",
    "dataframe_list = []\n",
    "percent_with_adaptor = 0\n",
    "\n",
    "for row in i7_table.iterrows():\n",
    "    _, i7_data = row\n",
    "    # prepare input and output file names for cutadapt\n",
    "    cutadapt_inputs = [\n",
    "        f'{raw_fastq_dir}/{i7_data.r2}',\n",
    "        f'{raw_fastq_dir}/{i7_data.r1}']\n",
    "    cutadapt_outputs = [\n",
    "        f'{demultiplexed_fastq_dir}/{i7_data.i7}.' + '{name}.R2.fastq.gz',\n",
    "        f'{demultiplexed_fastq_dir}/{i7_data.i7}.' + '{name}.R1.fastq.gz']\n",
    "    # generate a lookup table for cutadapt to pull inline barcodes from\n",
    "    samples_present = inline_table.loc[inline_table.in_i7 == i7_data.i7]\n",
    "    barcode_fasta = 'inline_barcodes.tmp.fasta'\n",
    "    with open(barcode_fasta, 'w') as f:\n",
    "        for title, seq in zip(samples_present.inline_barcode, samples_present.sequence):    \n",
    "            f.write(f'>{str(title)}\\n^{str(seq)}\\n')\n",
    "    command = f'cutadapt -e {inline_barcode_error_rate} --discard-untrimmed ' + \\\n",
    "              f'-g file:{barcode_fasta} -o {cutadapt_outputs[0]} ' + \\\n",
    "              f'-p {cutadapt_outputs[1]} {cutadapt_inputs[0]} {cutadapt_inputs[1]}'\n",
    "    # run cutadapt to split fastq files\n",
    "    output_string, _ = quickshell(\n",
    "        command,\n",
    "        print_output = True,\n",
    "        return_output = True,\n",
    "        output_path = f'{demultiplexed_fastq_dir}/{i7_data.title}_cutadapt_demultiplex.txt')\n",
    "\n",
    "    # record counts of each inline barcode found\n",
    "    percent_with_adaptor = (f\"In {cutadapt_inputs[0].split('/')[-1]} and mate:\\n  \" +\n",
    "        (' '.join([m for m in regex.finditer('Read 1 with adapter:.*\\n',output_string)][0].group()\n",
    "                  [:-1].split())).replace('1','2'))\n",
    "    barcode_counts = [int(m.group().split(' ')[1]) for m in regex.finditer('Trimmed: \\d* times', output_string)]\n",
    "    # generate output dataframe and append to list\n",
    "    data = np.asarray([\n",
    "        samples_present.title.values,\n",
    "        [i7_data.i7 for i in barcode_counts],\n",
    "        samples_present.inline_barcode.values,\n",
    "        barcode_counts]).T\n",
    "    output_df = pd.DataFrame(\n",
    "        columns=['title', 'i7', 'inline_barcode', 'counts'],\n",
    "        data=data)\n",
    "    dataframe_list.append(output_df)\n",
    "    # rename files to use filename from inline sample file\n",
    "    for sample_title, barcode in zip(samples_present.title.values, samples_present.inline_barcode.values):\n",
    "        temp_fastq_1 = f'{demultiplexed_fastq_dir}/{i7_data.i7}.{barcode}.R1.fastq.gz'\n",
    "        out_fastq_1 = f'{demultiplexed_fastq_dir}/{sample_title}.R1.fastq.gz'\n",
    "        !mv $temp_fastq_1 $out_fastq_1\n",
    "        temp_fastq_2 = f'{demultiplexed_fastq_dir}/{i7_data.i7}.{barcode}.R2.fastq.gz'\n",
    "        out_fastq_2 = f'{demultiplexed_fastq_dir}/{sample_title}.R2.fastq.gz'\n",
    "        !mv $temp_fastq_2 $out_fastq_2\n",
    "        \n",
    "# concatenate i7 dataframes\n",
    "output_info = pd.concat(dataframe_list, axis=0) \n",
    "output_info['percent_with_adaptor'] = percent_with_adaptor\n",
    "\n",
    "# write record csv\n",
    "output_info.to_csv(f'{demultiplexed_fastq_dir}/fastq_demultiplex_record.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4789b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Peter Culviner, PhD\n",
    "# iterate through file pairs and trim adapters\n",
    "trimming_log = f'{trimmed_fastq_dir}/split_trim_log.txt'\n",
    "\n",
    "with open(trimming_log, 'w') as f:\n",
    "    for row in inline_table.iterrows():\n",
    "        _, sample_data = row\n",
    "        # prepare input and output titles\n",
    "        cutadapt_inputs = [\n",
    "            f'{demultiplexed_fastq_dir}/{sample_data.title}.R1.fastq.gz',\n",
    "            f'{demultiplexed_fastq_dir}/{sample_data.title}.R2.fastq.gz']\n",
    "        cutadapt_outputs = [\n",
    "            f'{trimmed_fastq_dir}/{sample_data.title}.R1.fastq.gz',\n",
    "            f'{trimmed_fastq_dir}/{sample_data.title}.R2.fastq.gz']\n",
    "        # prepare cutadapt command\n",
    "        # read from read 1 (reverse transcription primer with barcode)\n",
    "        i7_adapter = f'{str(Seq(sample_data.sequence).reverse_complement())}{i7_trim}'\n",
    "        # read from read 2\n",
    "        i5_adapter = f'{i5_trim}'\n",
    "        command = f'cutadapt --overlap=1 --minimum-length={minimum_insert_length} -q {quality_threshold} ' + \\\n",
    "                  f'-j {threads} -a {i7_adapter} -A {i5_adapter} -o {cutadapt_outputs[0]} ' + \\\n",
    "                  f'-p {cutadapt_outputs[1]} {cutadapt_inputs[0]} {cutadapt_inputs[1]}'\n",
    "        # run cutadapt\n",
    "        output_trim, _ = quickshell(command, print_output=False, return_output=True)\n",
    "        # parse output to pull number of reads below minimum_trimmed_length\n",
    "        too_short = int(regex.search('too short:\\s*\\S*',output_trim).group().split(' ')[-1].replace(',',''))\n",
    "        # write full output to log\n",
    "        f.write(output_trim + '\\n' + 'too short: ' + too_short + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b6de82",
   "metadata": {},
   "source": [
    "## Remove UMIs and convert fastq to fasta for mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a335f09c",
   "metadata": {},
   "source": [
    "Even if you do not make subsequent use of unique molecular indices (UMIs), this step is important to remove them from the reads and enable accurate alignment.\n",
    "\n",
    "Update the UMI pattern below to enable UMI removal from both R1 and R2. The UMI will then be tacked onto the read ID, enabling de-duplication of .bam files with the dedup function from umi_tools after read mapping.\n",
    "\n",
    "The read ID will have the following structure after UMI extraction:\n",
    "\n",
    "ReadID_***NNNNN***NNNNN\n",
    "\n",
    ">***NNNNN*** = UMI from read1\n",
    "\n",
    ">NNNNN = UMI from read2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06e5d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through fastq files, split UMIs from reads and add to read_id,\n",
    "# and re-write reads as fasta\n",
    "\n",
    "# Initialize UMI length\n",
    "UMI_length = 5\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    R1_reads = f'{trimmed_fastq_dir}/{sample_data.title}.R1.fastq.gz'\n",
    "    R2_reads = f'{trimmed_fastq_dir}/{sample_data.title}.R2.fastq.gz'\n",
    "\n",
    "    counter = 0\n",
    "    # How many lines to store in memory at once\n",
    "    # Change if there are memory issues\n",
    "    lines_in = 50000000\n",
    "    \n",
    "    for chunk_R1 in pd.read_table(R1_reads, header=None, chunksize = lines_in):\n",
    "\n",
    "        R1 = pd.DataFrame(chunk_R1.values.reshape(-1, 4),\n",
    "                          columns=['read_id_R1', 'seq_R1', '+', 'qual'])\n",
    "        R1['read_id'] = R1['read_id_R1'].str[:-13]\n",
    "        R1['UMI_R1'] = R1['seq_R1'].str[0:UMI_length]\n",
    "        R1['read_R1'] = R1['seq_R1'].str[UMI_length:]\n",
    "        R1['qual_R1'] = R1['qual'].str[UMI_length:]\n",
    "        \n",
    "        R2 = pd.DataFrame(pd.read_table(R2_reads, header=None, skiprows = counter * lines_in,\n",
    "                                        nrows = lines_in).values.reshape(-1, 4),\n",
    "                      columns=['read_id_R2', 'seq_R2', '+', 'qual'])\n",
    "        R2['read_id'] = R2['read_id_R2'].str[:-13]\n",
    "        R2['UMI_R2'] = R2['seq_R2'].str[0:UMI_length]\n",
    "        R2['read_R2'] = R2['seq_R2'].str[UMI_length:]\n",
    "        R2['qual_R2'] = R2['qual'].str[UMI_length:]\n",
    "\n",
    "        all_reads = pd.merge(R1[['read_id','seq_R1','UMI_R1','read_R1','qual_R1','read_id_R1']],\n",
    "                             R2[['read_id','seq_R2','UMI_R2','read_R2','qual_R2','read_id_R2']],\n",
    "                             on = 'read_id')\n",
    "    \n",
    "        all_reads['read_id_UMI'] = all_reads['read_id'] + '_' + \\\n",
    "                                   all_reads['UMI_R1'] + all_reads['UMI_R2']\n",
    "\n",
    "        fasta_R1 = all_reads.read_id_UMI + '\\n' + all_reads.read_R1\n",
    "        fasta_R2 = all_reads.read_id_UMI + '\\n' + all_reads.read_R2\n",
    "\n",
    "        output_R1 = f'{noUMI_dir}/{sample_data.title}.R1.{counter}.fasta'\n",
    "        output_R2 = f'{noUMI_dir}/{sample_data.title}.R2.{counter}.fasta'\n",
    "\n",
    "        # Save this table for UMI analysis after read mapping\n",
    "        all_reads.to_csv(f'{noUMI_dir}/{sample_data.title}.UMI_readID.csv')\n",
    "                     \n",
    "        fasta_R1.to_csv(output_R1, index=False, quoting=csv.QUOTE_NONE, escapechar = \"(\", header=None)\n",
    "        fasta_R2.to_csv(output_R2, index=False, quoting=csv.QUOTE_NONE, escapechar = \"(\", header=None)\n",
    "        print(f'Chunking UMIs: {sample_data.title} iteration {counter} done')\n",
    "        counter += 1\n",
    "        \n",
    "    print(f'Extracting UMIs: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0864b2",
   "metadata": {},
   "source": [
    "# Format FASTAs for mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea33ca3",
   "metadata": {},
   "source": [
    "The fasta files are written with escape character `(` to enable proper export, but this character must be removed prior to fasta mapping.\n",
    "\n",
    "Run the following command to remove escape character `(` in the fasta files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08aced3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_command = f'sed -i s/\\(//g {noUMI_dir}/*.fasta'\n",
    "\n",
    "replace = quickshell(replace_command, print_output = False, return_output = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d60c13",
   "metadata": {},
   "source": [
    "# Combine FASTAs from same sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29aa4957",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    \n",
    "    R1_string = f'{noUMI_dir}/{sample_data.title}.R1.0.fasta'\n",
    "    R2_string = f'{noUMI_dir}/{sample_data.title}.R2.0.fasta'\n",
    "    \n",
    "    for file in listdir(f'{noUMI_dir}'):\n",
    "        if file.startswith(f'{sample_data.title}.R1'):\n",
    "            R1_string = f'{R1_string} + \" \" + {noUMI_dir}/{file}'\n",
    "        elif file.startswith(f'{sample_data.title}.R2'):\n",
    "            R2_string = f'{R2_string} + \" \" + {noUMI_dir}/{file}'\n",
    "            \n",
    "    R1_combine_command = f'cat {R1_string} > {combined_fasta_dir}/{sample_data.title}.R1.fasta'\n",
    "    R2_combine_command = f'cat {R2_string} > {combined_fasta_dir}/{sample_data.title}.R2.fasta'\n",
    "    \n",
    "    R1_combine = quickshell(R1_combine_command, print_output = False, return_output = False)\n",
    "    print(f'Combining FASTAs: {sample_data.title} R1 done')\n",
    "    R2_combine = quickshell(R2_combine_command, print_output = False, return_output = False)\n",
    "    print(f'Combining FASTAs: {sample_data.title} R2 done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4763b",
   "metadata": {},
   "source": [
    "# Mapping reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c013501d",
   "metadata": {},
   "source": [
    "Map the reads to a concatenated genome: a spike genome (Eco) and an experimental genome of interest (Mtb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a95d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index the input genomes (only needs to be done once)\n",
    "index_genome_command = f'bwa index {genome_dir}/Eco_Mtb_genome.fasta'\n",
    "\n",
    "index_genome = quickshell(index_genome_command, print_output = True, return_output = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49050856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BWA to align to concatenated genomes \n",
    "# for spike calculation and subsequent end enrichment analysis\n",
    "\n",
    "# bwa algorithm. Mem is default for most applications, read documentation to decide\n",
    "bwa_algorithm = 'mem'\n",
    "\n",
    "# Set print_output = True to see command-line output\n",
    "# Set return_output = False if assigning commmand-line output to a variable\n",
    "print_output = False\n",
    "return_output = False\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    R1_reads = f'{combined_fasta_dir}/{sample_data.title}.R1.fasta'\n",
    "    R2_reads = f'{combined_fasta_dir}/{sample_data.title}.R2.fasta'\n",
    "\n",
    "    map_command = f'bwa {bwa_algorithm} -t {threads} {genome_dir}/Eco_Mtb_genome.fasta ' + \\\n",
    "                       f'{R1_reads} {R2_reads} > {initial_alignments_dir}/{sample_data.title}.sam'\n",
    "    \n",
    "    map_output = quickshell(map_command, print_output = print_output, return_output = return_output)\n",
    "    print(f'Initial alignments: {sample_data.title} Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ae74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process alignments: \n",
    "# sort, convert to bam, and index\n",
    "\n",
    "# Set print_output = True to see command-line output\n",
    "# Set return_output = False if assigning commmand-line output to a variable\n",
    "print_output = False\n",
    "return_output = False\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # Sort sam file and output as bam\n",
    "    sort_command = f'samtools sort -O BAM {initial_alignments_dir}/{sample_data.title}.sam > ' + \\\n",
    "                          f'{initial_alignments_dir}/sorted_{sample_data.title}.bam'\n",
    "    \n",
    "    sort_output = quickshell(sort_command, print_output = print_output, return_output=return_output)\n",
    "    \n",
    "    # Index sorted bam\n",
    "    index_command = f'samtools index {initial_alignments_dir}/sorted_{sample_data.title}.bam'\n",
    "    \n",
    "    index_output = quickshell(index_command, print_output = print_output, return_output=return_output)\n",
    "\n",
    "    print(f'Sort and index alignments before de-duplication: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87157b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mapping statistics BEFORE de-duplication\n",
    "\n",
    "# List to generate dataframe with spike counts for normalization (needed in downstream analysis)\n",
    "# and other mapping statistics (not needed)\n",
    "# mapping_stats = []\n",
    "\n",
    "# Update with the name of your genome and relevant regions\n",
    "Eco_region = 'Eco_Mtb:1-4641652'\n",
    "Mtb_region = 'Eco_Mtb:4641653-9053361'\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # Extract mapping stats\n",
    "    count_Mtb_reads_command = f'samtools view -c -F 260 ' + \\\n",
    "                              f'{initial_alignments_dir}/sorted_{sample_data.title}.bam \"{Mtb_region}\"'\n",
    "    count_Mtb_reads = int(quickshell(count_Mtb_reads_command,\n",
    "                                     print_output = False,\n",
    "                                     return_output=True)[0].split('\\n')[0])\n",
    "    \n",
    "    count_all_mapped_reads_command = f'samtools view -c -F 260 ' + \\\n",
    "                                         f'{initial_alignments_dir}/sorted_{sample_data.title}.bam'\n",
    "    count_all_mapped_reads = int(quickshell(count_all_mapped_reads_command,\n",
    "                                                print_output = False,\n",
    "                                                return_output=True)[0].split('\\n')[0])\n",
    "    Mtb_percent = (count_Mtb_reads / count_all_mapped_reads) * 100\n",
    "    \n",
    "    count_Eco_reads_command = f'samtools view -c -F 260 ' + \\\n",
    "                                f'{initial_alignments_dir}/sorted_{sample_data.title}.bam \"{Eco_region}\"'\n",
    "    count_Eco_reads = int(quickshell(count_Eco_reads_command,\n",
    "                                       print_output = False,\n",
    "                                       return_output=True)[0].split('\\n')[0])\n",
    "\n",
    "    Eco_percent = (count_Eco_reads / count_all_mapped_reads) * 100\n",
    "    \n",
    "    count_unmapped_reads_command = f'samtools view -c -f 4 ' + \\\n",
    "                                       f'{initial_alignments_dir}/sorted_{sample_data.title}.bam'\n",
    "    count_unmapped_reads = int(quickshell(count_unmapped_reads_command,\n",
    "                                              print_output = False,\n",
    "                                              return_output=True)[0].split('\\n')[0])\n",
    "    \n",
    "    \n",
    "    count_all_reads_command = f'samtools view -c ' + \\\n",
    "                                  f'{initial_alignments_dir}/sorted_{sample_data.title}_enrich.bam'\n",
    "    count_all_reads = int(quickshell(count_all_end_reads_command,\n",
    "                                         print_output = False,\n",
    "                                         return_output=True)[0].split('\\n')[0])\n",
    "    unmapped_percent = (count_unmapped_reads / count_all_reads) * 100\n",
    "    \n",
    "    mapping_stats.append([sample_data.title, count_Eco_reads, Eco_percent, count_Mtb_reads,\n",
    "                          Mtb_percent, unmapped_percent])\n",
    "    \n",
    "    print(f'Mapping stats before de-duplication: {sample_data.title} done')\n",
    "\n",
    "mapping_DF = pd.DataFrame(mapping_stats, columns = ['Sample_Name','Eco Counts',\n",
    "                                                    'Eco % Mapped','Mtb Counts','Mtb % Mapped',\n",
    "                                                    '% Unmapped'])\n",
    "mapping_DF.to_csv(f'{initial_alignments_dir}/mapping_stats_preDeDup.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47fc13c",
   "metadata": {},
   "source": [
    "# De-duplication: remove reads with same UMIs and mapping locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5bb988a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De-duplicating alignments: core1 done\n",
      "De-duplicating alignments: core2 done\n",
      "De-duplicating alignments: core3 done\n",
      "De-duplicating alignments: RC1 done\n",
      "De-duplicating alignments: RC2 done\n",
      "De-duplicating alignments: RC3 done\n",
      "De-duplicating alignments: WhiB1 done\n",
      "De-duplicating alignments: WhiB2 done\n",
      "De-duplicating alignments: WhiB3 done\n",
      "De-duplicating alignments: core_CRP1 done\n",
      "De-duplicating alignments: core_CRP2 done\n",
      "De-duplicating alignments: core_CRP3 done\n",
      "De-duplicating alignments: RC_CRP1 done\n",
      "De-duplicating alignments: RC_CRP2 done\n",
      "De-duplicating alignments: RC_CRP3 done\n",
      "De-duplicating alignments: CRP1 done\n",
      "De-duplicating alignments: CRP2 done\n",
      "De-duplicating alignments: CRP3 done\n"
     ]
    }
   ],
   "source": [
    "print_output = False\n",
    "return_output = False\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    dedup_command = f'umi_tools dedup -I {initial_alignments_dir}/sorted_{sample_data.title}.bam ' + \\\n",
    "                           f'--paired --output-stats={dedup_logs_dir}/{sample_data.title} ' + \\\n",
    "                           f'--no-sort-output -S {dedup_alignments_dir}/{sample_data.title}.bam'\n",
    "    dedup_output = quickshell(dedup_command,\n",
    "                             print_output = print_output,\n",
    "                             return_output = return_output)\n",
    "    print(f'De-duplicating alignments: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f1387deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting and indexing de-duplicated alignments: core1 done\n",
      "Sorting and indexing de-duplicated alignments: core2 done\n",
      "Sorting and indexing de-duplicated alignments: core3 done\n",
      "Sorting and indexing de-duplicated alignments: RC1 done\n",
      "Sorting and indexing de-duplicated alignments: RC2 done\n",
      "Sorting and indexing de-duplicated alignments: RC3 done\n",
      "Sorting and indexing de-duplicated alignments: WhiB1 done\n",
      "Sorting and indexing de-duplicated alignments: WhiB2 done\n",
      "Sorting and indexing de-duplicated alignments: WhiB3 done\n",
      "Sorting and indexing de-duplicated alignments: core_CRP1 done\n",
      "Sorting and indexing de-duplicated alignments: core_CRP2 done\n",
      "Sorting and indexing de-duplicated alignments: core_CRP3 done\n",
      "Sorting and indexing de-duplicated alignments: RC_CRP1 done\n",
      "Sorting and indexing de-duplicated alignments: RC_CRP2 done\n",
      "Sorting and indexing de-duplicated alignments: RC_CRP3 done\n",
      "Sorting and indexing de-duplicated alignments: CRP1 done\n",
      "Sorting and indexing de-duplicated alignments: CRP2 done\n",
      "Sorting and indexing de-duplicated alignments: CRP3 done\n"
     ]
    }
   ],
   "source": [
    "# Process deduplicated alignments: \n",
    "# sort and index\n",
    "\n",
    "# Set print_output = True to see command-line output\n",
    "# Set return_output = False if assigning commmand-line output to a variable\n",
    "print_output = False\n",
    "return_output = False\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # Sort sam file and output as bam\n",
    "    sort_dedup_command = f'samtools sort -O BAM {dedup_alignments_dir}/{sample_data.title}.bam > ' + \\\n",
    "                          f'{dedup_alignments_dir}/sorted_{sample_data.title}.bam'\n",
    "\n",
    "    sort_dedup_output = quickshell(sort_dedup_command, print_output = print_output, return_output=return_output)\n",
    "    \n",
    "    # Index sorted bam\n",
    "    index_dedup_command = f'samtools index {dedup_alignments_dir}/sorted_{sample_data.title}.bam'\n",
    "    \n",
    "    index_dedup_output = quickshell(index_dedup_command, print_output = print_output, return_output=return_output)\n",
    "    print(f'Sorting and indexing de-duplicated alignments: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f27e47c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping stats after de-duplication: core1 done\n",
      "Mapping stats after de-duplication: core2 done\n",
      "Mapping stats after de-duplication: core3 done\n",
      "Mapping stats after de-duplication: RC1 done\n",
      "Mapping stats after de-duplication: RC2 done\n",
      "Mapping stats after de-duplication: RC3 done\n",
      "Mapping stats after de-duplication: WhiB1 done\n",
      "Mapping stats after de-duplication: WhiB2 done\n",
      "Mapping stats after de-duplication: WhiB3 done\n",
      "Mapping stats after de-duplication: core_CRP1 done\n",
      "Mapping stats after de-duplication: core_CRP2 done\n",
      "Mapping stats after de-duplication: core_CRP3 done\n",
      "Mapping stats after de-duplication: RC_CRP1 done\n",
      "Mapping stats after de-duplication: RC_CRP2 done\n",
      "Mapping stats after de-duplication: RC_CRP3 done\n",
      "Mapping stats after de-duplication: CRP1 done\n",
      "Mapping stats after de-duplication: CRP2 done\n",
      "Mapping stats after de-duplication: CRP3 done\n"
     ]
    }
   ],
   "source": [
    "# Generate mapping statistics AFTER de-duplication\n",
    "\n",
    "# List to generate dataframe with spike counts for normalization (needed in downstream analysis)\n",
    "# and other mapping statistics (not needed)\n",
    "# mapping_stats = []\n",
    "\n",
    "# Update with the name of your genome and relevant regions\n",
    "Eco_region = 'Eco_Mtb:1-4641652'\n",
    "Mtb_region = 'Eco_Mtb:4641653-9053361'\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # Extract mapping stats\n",
    "    count_Mtb_reads_command = f'samtools view -c -F 260 ' + \\\n",
    "                              f'{dedup_alignments_dir}/sorted_{sample_data.title}.bam \"{Mtb_region}\"'\n",
    "    count_Mtb_reads = int(quickshell(count_Mtb_reads_command,\n",
    "                                     print_output = False,\n",
    "                                     return_output=True)[0].split('\\n')[0])\n",
    "    \n",
    "    count_all_mapped_reads_command = f'samtools view -c -F 260 ' + \\\n",
    "                                         f'{dedup_alignments_dir}/sorted_{sample_data.title}.bam'\n",
    "    count_all_mapped_reads = int(quickshell(count_all_mapped_reads_command,\n",
    "                                                print_output = False,\n",
    "                                                return_output=True)[0].split('\\n')[0])\n",
    "    Mtb_percent = (count_Mtb_reads / count_all_mapped_reads) * 100\n",
    "    \n",
    "    count_Eco_reads_command = f'samtools view -c -F 260 ' + \\\n",
    "                                f'{dedup_alignments_dir}/sorted_{sample_data.title}.bam \"{Eco_region}\"'\n",
    "    count_Eco_reads = int(quickshell(count_Eco_reads_command,\n",
    "                                       print_output = False,\n",
    "                                       return_output=True)[0].split('\\n')[0])\n",
    "\n",
    "    Eco_percent = (count_Eco_reads / count_all_mapped_reads) * 100\n",
    "    \n",
    "    count_unmapped_reads_command = f'samtools view -c -f 4 ' + \\\n",
    "                                       f'{dedup_alignments_dir}/sorted_{sample_data.title}.bam'\n",
    "    count_unmapped_reads = int(quickshell(count_unmapped_reads_command,\n",
    "                                              print_output = False,\n",
    "                                              return_output=True)[0].split('\\n')[0])\n",
    "    \n",
    "    \n",
    "    count_all_reads_command = f'samtools view -c ' + \\\n",
    "                                  f'{dedup_alignments_dir}/sorted_{sample_data.title}_enrich.bam'\n",
    "    count_all_reads = int(quickshell(count_all_end_reads_command,\n",
    "                                         print_output = False,\n",
    "                                         return_output=True)[0].split('\\n')[0])\n",
    "    unmapped_percent = (count_unmapped_reads / count_all_reads) * 100\n",
    "    \n",
    "    mapping_stats.append([sample_data.title, count_Eco_reads, Eco_percent, count_Mtb_reads,\n",
    "                          Mtb_percent, unmapped_percent])\n",
    "    \n",
    "    print(f'Mapping stats after de-duplication: {sample_data.title} done')\n",
    "\n",
    "mapping_DF = pd.DataFrame(mapping_stats, columns = ['Sample_Name','Eco Counts',\n",
    "                                                    'Eco % Mapped','Mtb Counts','Mtb % Mapped',\n",
    "                                                    '% Unmapped'])\n",
    "mapping_DF.to_csv(f'{dedup_alignments_dir}/mapping_stats_postDeDup.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a07ca1",
   "metadata": {},
   "source": [
    "# Extract only Mtb R2 reads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e0b8e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate and index R2-only alignments: core1 done\n",
      "Generate and index R2-only alignments: core2 done\n",
      "Generate and index R2-only alignments: core3 done\n",
      "Generate and index R2-only alignments: RC1 done\n",
      "Generate and index R2-only alignments: RC2 done\n",
      "Generate and index R2-only alignments: RC3 done\n",
      "Generate and index R2-only alignments: WhiB1 done\n",
      "Generate and index R2-only alignments: WhiB2 done\n",
      "Generate and index R2-only alignments: WhiB3 done\n",
      "Generate and index R2-only alignments: core_CRP1 done\n",
      "Generate and index R2-only alignments: core_CRP2 done\n",
      "Generate and index R2-only alignments: core_CRP3 done\n",
      "Generate and index R2-only alignments: RC_CRP1 done\n",
      "Generate and index R2-only alignments: RC_CRP2 done\n",
      "Generate and index R2-only alignments: RC_CRP3 done\n",
      "Generate and index R2-only alignments: CRP1 done\n",
      "Generate and index R2-only alignments: CRP2 done\n",
      "Generate and index R2-only alignments: CRP3 done\n"
     ]
    }
   ],
   "source": [
    "# Set print_output = True to see command-line output\n",
    "# Set return_output = False if assigning commmand-line output to a variable\n",
    "print_output = False\n",
    "return_output = False\n",
    "\n",
    "Mtb_region = 'Eco_Mtb:4641653-9053361'\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # Output the R2 reads mapping to the genome with enriched ends to analyze  \n",
    "    ref_R2_command = f'samtools view -b -f 0x0080 ' + \\\n",
    "                     f'{dedup_alignments_dir}/sorted_{sample_data.title}.bam ' + \\\n",
    "                     f'\"{Mtb_region}\" -o {R2_alignments_dir}/{sample_data.title}_R2.bam'\n",
    "    ref_R2 = quickshell(ref_R2_command, print_output = print_output, return_output = return_output)\n",
    "    \n",
    "    # Index the R2 only bam\n",
    "    index_R2_command = f'samtools index {R2_alignments_dir}/{sample_data.title}_R2.bam'\n",
    "    index_R2 = quickshell(index_R2_command, print_output = print_output, return_output = return_output)\n",
    "    print(f'Generate and index R2-only alignments: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and assemble all R2 reads to decide on number of reads to downsample to\n",
    "R2_reads = []\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    count_R2_reads_command = f'samtools view -c ' + \\\n",
    "                                  f'{R2_alignments_dir}/{sample_data.title}_R2.bam'\n",
    "    count_R2_reads = int(quickshell(count_R2_reads_command,\n",
    "                                         print_output = False,\n",
    "                                         return_output=True)[0].split('\\n')[0])\n",
    "    R2_reads.append([sample_data.title, count_R2_reads])\n",
    "    print(f'{sample_data.title} done')\n",
    "    \n",
    "reads_DF = pd.DataFrame(R2_reads, columns = ['Sample_Name','R2_read_counts'])\n",
    "\n",
    "reads_DF['Condition'] = reads_DF['Sample_Name'].str[:-1]\n",
    "conditions = reads_DF['Condition'].unique()\n",
    "reads_DF['Rank'] = 0\n",
    "\n",
    "DF_list = []\n",
    "\n",
    "for i in range(len(conditions)):\n",
    "    replicates = reads_DF.loc[reads_DF['Condition'] == conditions[i]]\n",
    "    replicates.loc[replicates['R2_read_counts'] == replicates['R2_read_counts'].max(), 'Rank'] = 1\n",
    "    replicates.loc[replicates['R2_read_counts'] == replicates['R2_read_counts'].min(), 'Rank'] = 3\n",
    "    replicates.loc[replicates['Rank'] == 0, 'Rank'] = 2\n",
    "    DF_list.append(replicates)\n",
    "    \n",
    "reads_DF_ranked = pd.concat(DF_list)\n",
    "\n",
    "# Inspect read depths to decide on a minimum read depth to downsample all samples for subsequent analysis\n",
    "reads_DF_ranked.to_csv(f'{R2_alignments_dir}/R2_read_counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb48bad1",
   "metadata": {},
   "source": [
    "# Extract only R2 reads from Eco samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61ce83c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print_output = True to see command-line output\n",
    "# Set return_output = False if assigning commmand-line output to a variable\n",
    "print_output = True\n",
    "return_output = False\n",
    "\n",
    "Eco_region = 'Eco_Mtb:1-4641652'\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # Output the spike R2 reads\n",
    "    spike_R2_command = f'samtools view -b -f 0x0080 ' + \\\n",
    "                     f'{dedup_alignments_dir}/sorted_{sample_data.title}.bam ' + \\\n",
    "                     f'\"{Eco_region}\" -o {spike_R2_alignments_dir}/{sample_data.title}_R2.bam'\n",
    "    spike_R2 = quickshell(spike_R2_command, print_output = print_output, return_output = return_output)\n",
    "    \n",
    "    # Index the R2 only bam\n",
    "    index_spike_R2_command = f'samtools index {spike_R2_alignments_dir}/{sample_data.title}_R2.bam'\n",
    "    index_spike_R2 = quickshell(index_spike_R2_command, print_output = print_output, return_output = return_output)\n",
    "    print(f'Generate and index R2-only spike alignments: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23206d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count and assemble all R2 spike reads for downstream spike analysis scaled with downsampling\n",
    "R2_reads = []\n",
    "\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    count_R2_reads_command = f'samtools view -c ' + \\\n",
    "                                  f'{spike_R2_alignments_dir}/{sample_data.title}_R2.bam'\n",
    "    count_R2_reads = int(quickshell(count_R2_reads_command,\n",
    "                                         print_output = False,\n",
    "                                         return_output=True)[0].split('\\n')[0])\n",
    "    R2_reads.append([sample_data.title, count_R2_reads])\n",
    "    print(f'{sample_data.title} spike done')\n",
    "    \n",
    "reads_DF = pd.DataFrame(R2_reads, columns = ['Sample_Name','R2_read_counts'])\n",
    "\n",
    "reads_DF['Condition'] = reads_DF['Sample_Name'].str[:-1]\n",
    "conditions = reads_DF['Condition'].unique()\n",
    "reads_DF['Rank'] = 0\n",
    "\n",
    "DF_list = []\n",
    "\n",
    "for i in range(len(conditions)):\n",
    "    replicates = reads_DF.loc[reads_DF['Condition'] == conditions[i]]\n",
    "    replicates.loc[replicates['R2_read_counts'] == replicates['R2_read_counts'].max(), 'Rank'] = 1\n",
    "    replicates.loc[replicates['R2_read_counts'] == replicates['R2_read_counts'].min(), 'Rank'] = 3\n",
    "    replicates.loc[replicates['Rank'] == 0, 'Rank'] = 2\n",
    "    DF_list.append(replicates)\n",
    "    \n",
    "reads_DF_ranked = pd.concat(DF_list)\n",
    "\n",
    "# Inspect read depths to decide on a minimum read depth to downsample all samples for subsequent analysis\n",
    "reads_DF_ranked.to_csv(f'{spike_R2_alignments_dir}/R2_read_counts.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
