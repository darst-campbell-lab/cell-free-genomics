{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0aceab8",
   "metadata": {},
   "source": [
    "## Cell-Free Genomics: Identify Enriched Ends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0304c289",
   "metadata": {},
   "source": [
    "#### Ruby Froom, Campbell/Darst and Rock labs at Rockefeller University\n",
    "\n",
    "The following pipeline downsamples, calls genomic coordinates where of enriched RNA ends (5' or 3', depending on the library preparation) occur. \n",
    "\n",
    "Other notebooks in `darst-campbell-lab` Github contain 5' or 3' end-specific interactive workflows for filtering of these coordinates, motif searching, and sequence-feature annotation. \n",
    "The goal of these notebooks is to reveal *cis* (promoter features for 5' ends; RNA structure and elemental pauses for 3' ends) and *trans* (TF binding regions) determinants of transcription regulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb62b29",
   "metadata": {},
   "source": [
    "## Modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e32b2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import exists\n",
    "import scipy\n",
    "from statsmodels.stats.multitest import fdrcorrection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "255ffceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by Peter Culviner, PhD to enable command-line access through Jupyter\n",
    "def quickshell(command, print_output=True, output_path=None, return_output=False):\n",
    "    process_output = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout = process_output.stdout.decode('utf-8')\n",
    "    stderr = process_output.stderr.decode('utf-8')\n",
    "    output_string = f'STDOUT:\\n{stdout}\\nSTDERR:\\n{stderr}\\n'\n",
    "    if print_output:\n",
    "        print('$ ' + command)\n",
    "        print(output_string)\n",
    "    if output_path is not None:\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(output_string)\n",
    "    if return_output:\n",
    "        return stdout, stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c429e4e0",
   "metadata": {},
   "source": [
    "Inputs for directory creation and pointing to files. \n",
    "\n",
    "The starting requirements for this pipeline are:\n",
    ">**main_path**: the absolute path of the working directory. Update to reflect your own configuration.\n",
    "\n",
    ">**input_csv_dir:** a directory containing `i7_sample_file` and `inline_sample_file` csv files (see below for file specifications) called `input_csv_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fac0bd87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>r1</th>\n",
       "      <th>r2</th>\n",
       "      <th>i7</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frag_gDNA_3enrich1_S1_R1_001.fastq.gz</td>\n",
       "      <td>frag_gDNA_3enrich1_S1_R2_001.fastq.gz</td>\n",
       "      <td>1</td>\n",
       "      <td>gDNA1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>frag_gDNA_3enrich2_S2_R1_001.fastq.gz</td>\n",
       "      <td>frag_gDNA_3enrich2_S2_R2_001.fastq.gz</td>\n",
       "      <td>2</td>\n",
       "      <td>gDNA2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>frag_gDNA_3enrich3_S3_R1_001.fastq.gz</td>\n",
       "      <td>frag_gDNA_3enrich3_S3_R2_001.fastq.gz</td>\n",
       "      <td>3</td>\n",
       "      <td>gDNA3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      r1  \\\n",
       "0  frag_gDNA_3enrich1_S1_R1_001.fastq.gz   \n",
       "1  frag_gDNA_3enrich2_S2_R1_001.fastq.gz   \n",
       "2  frag_gDNA_3enrich3_S3_R1_001.fastq.gz   \n",
       "\n",
       "                                      r2  i7  title  \n",
       "0  frag_gDNA_3enrich1_S1_R2_001.fastq.gz   1  gDNA1  \n",
       "1  frag_gDNA_3enrich2_S2_R2_001.fastq.gz   2  gDNA2  \n",
       "2  frag_gDNA_3enrich3_S3_R2_001.fastq.gz   3  gDNA3  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initializing locations of input .csv files, etc. \n",
    "main_path = '5enrich_CRP'\n",
    "#main_path = '3enrich_NusAG'\n",
    "main_path = 'gDNA'\n",
    "\n",
    "input_csv_dir = f'{main_path}/input_csv_files'\n",
    "\n",
    "#sample_table = pd.read_csv(f'{input_csv_dir}/inline_barcodes_5enrich.csv')\n",
    "sample_table = pd.read_csv(f'{input_csv_dir}/inline_barcodes_3enrich.csv')\n",
    "#sample_table = pd.read_csv(f'{input_csv_dir}/i7_barcodes_gDNA.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11067e3",
   "metadata": {},
   "source": [
    "The additional directories below are a suggested organization for subsequent processing steps in this notebook. Assumes that the mode of enriched end calling will be `bootstrap` (see __ section below).\n",
    "\n",
    "The cells below will initialize these variables and create the directories inside the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27786b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for RNA processing\n",
    "\n",
    "# Read prep dirs\n",
    "alignments_dir = f'{main_path}/readPrep/dedup_alignments'\n",
    "R2_alignments_dir = f'{main_path}/readPrep/R2_alignments'\n",
    "spike_R2_alignments_dir = f'{main_path}/readPrep/spike_R2_alignments'\n",
    "\n",
    "# Extracting single-bp info from alignments\n",
    "identifyEnrichedEnds_dir = f'{main_path}/identifyEnrichedEnds'\n",
    "downsample_dir = f'{identifyEnrichedEnds_dir}/downsampled_R2_bams'\n",
    "bigWig_dir = f'{identifyEnrichedEnds_dir}/bigWigs'\n",
    "ends_dir = f'{identifyEnrichedEnds_dir}/ends'\n",
    "coverage_dir = f'{identifyEnrichedEnds_dir}/coverage'\n",
    "bootstrap_calls_dir = f'{identifyEnrichedEnds_dir}/bootstrap_calls'\n",
    "pre_blackList_dir = f'{identifyEnrichedEnds_dir}/consensus_calls_preBlackList'\n",
    "post_blackList_dir = f'{identifyEnrichedEnds_dir}/consensus_calls_postBlackList'\n",
    "\n",
    "# Same but for spike\n",
    "spike_downsample_dir = f'{identifyEnrichedEnds_dir}/downsampled_spike_R2_bams'\n",
    "spike_bigWig_dir = f'{identifyEnrichedEnds_dir}/bigWigs_spike'\n",
    "spike_ends_dir = f'{identifyEnrichedEnds_dir}/ends_spike'\n",
    "spike_coverage_dir = f'{identifyEnrichedEnds_dir}/cov_spike'\n",
    "spike_bootstrap_calls_dir = f'{identifyEnrichedEnds_dir}/bootstrap_calls_spike'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2874663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for gDNA processing\n",
    "alignments_dir = f'{main_path}/readPrep/alignments'\n",
    "R2_alignments_dir = f'{main_path}/readPrep/R2_alignments'\n",
    "spike_R2_alignments_dir = f'{main_path}/readPrep/spike_R2_alignments'\n",
    "\n",
    "# Extracting single-bp info from alignments\n",
    "identifyEnrichedEnds_dir = f'{main_path}/identifyEnrichedEnds'\n",
    "downsample_dir = f'{identifyEnrichedEnds_dir}/downsampled_R2_bams'\n",
    "bigWig_dir = f'{identifyEnrichedEnds_dir}/bigWigs'\n",
    "ends_dir = f'{identifyEnrichedEnds_dir}/ends'\n",
    "coverage_dir = f'{identifyEnrichedEnds_dir}/coverage'\n",
    "bootstrap_calls_dir = f'{identifyEnrichedEnds_dir}/bootstrap_calls'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c4f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $identifyEnrichedEnds_dir\n",
    "!mkdir $downsample_dir\n",
    "!mkdir $bigWig_dir\n",
    "!mkdir $ends_dir\n",
    "!mkdir $coverage_dir\n",
    "!mkdir $bootstrap_calls_dir\n",
    "\n",
    "!mkdir $spike_downsample_dir\n",
    "!mkdir $spike_bigWig_dir\n",
    "!mkdir $spike_ends_dir\n",
    "!mkdir $spike_coverage_dir\n",
    "!mkdir $spike_bootstrap_calls_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d214cb",
   "metadata": {},
   "source": [
    "# Calling enriched ends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cf0ba6",
   "metadata": {},
   "source": [
    "This section makes use of a script written by Mike Wolfe, PhD (Landick lab) to analyze single-bp resolution data.\n",
    "\n",
    "Enriched ends are called based on a bootstrapping algorithm, where counts are shuffled within a user-defined window flanking every genomic coordinate for 10,000 iterations to manually assign p-values to every position in the genome (calculated as # iterations where random value > actual value, divided by 10,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78da995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate how many CPUs you want to use\n",
    "processors = 10\n",
    "\n",
    "## For both: err on the side of inclusive, because further filtering will be carried out\n",
    "# Choose starting minimum coverage value to call an enriched transcript end\n",
    "cov_cutoff = 2\n",
    "\n",
    "# Choose maximum q-value to include a peak in the output of the bootstrap calling\n",
    "alpha = 1\n",
    "\n",
    "# Number of bp on either side of potential peak to consider for bootstrapping\n",
    "window = 100\n",
    "\n",
    "# Number of replicates per sample\n",
    "replicates = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f7f08",
   "metadata": {},
   "source": [
    "## Downsample R2 reads (experimental and spike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "203d8caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_depths = pd.read_csv(f'{R2_alignments_dir}/R2_read_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac83fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_output = False\n",
    "return_output = False\n",
    "\n",
    "read_depths = pd.read_csv(f'{R2_alignments_dir}/R2_read_counts.csv')\n",
    "\n",
    "# Then, set the sample_table to a dataframe that only contains the samples you'll be analyzing\n",
    "# Change if need be (i.e. one sample has read depths that are too low)\n",
    "# Here, the downsample depth is the minimum read depth across all samples. Can change if need be\n",
    "downsample_depth = read_depths['R2_read_counts'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27bfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_depths = []\n",
    "\n",
    "for row in sample_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    \n",
    "    original_depth = read_depths.loc[read_depths['Sample_Name'] == sample_data.title, 'R2_read_counts']\n",
    "    downsample_command = f'samtools view -b -s {(downsample_depth/int(original_depth))} ' + \\\n",
    "                         f'{R2_alignments_dir}/{sample_data.title}_R2.bam > ' + \\\n",
    "                         f'{downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    downsample = quickshell(downsample_command,\n",
    "                            print_output = print_output,\n",
    "                            return_output = return_output)\n",
    "    \n",
    "    # Index the downsampled R2 only bam\n",
    "    index_downsample_command = f'samtools index {downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    index_downsample = quickshell(index_downsample_command,\n",
    "                                  print_output = print_output,\n",
    "                                  return_output = return_output)\n",
    "    \n",
    "    count_downsample_command = f'samtools view -c ' + \\\n",
    "                                  f'{downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    count_downsample = int(quickshell(count_downsample_command,\n",
    "                                         print_output = False,\n",
    "                                         return_output=True)[0].split('\\n')[0])\n",
    "    downsampled_depths.append([sample_data.title, count_downsample])\n",
    "    \n",
    "    # Spike downsampling\n",
    "    spike_downsample_command = f'samtools view -b -s {(downsample_depth/int(original_depth))} ' + \\\n",
    "                         f'{spike_R2_alignments_dir}/{sample_data.title}_R2.bam > ' + \\\n",
    "                         f'{spike_downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    spike_downsample = quickshell(spike_downsample_command,\n",
    "                            print_output = print_output,\n",
    "                            return_output = return_output)\n",
    "    \n",
    "    # Index the downsampled R2 only bam\n",
    "    index_spike_downsample_command = f'samtools index {spike_downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    index_spike_downsample = quickshell(index_spike_downsample_command,\n",
    "                                  print_output = print_output,\n",
    "                                  return_output = return_output)\n",
    "    \n",
    "    count_spike_downsample_command = f'samtools view -c ' + \\\n",
    "                                  f'{spike_downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    count_spike_downsample = int(quickshell(count_spike_downsample_command,\n",
    "                                         print_output = False,\n",
    "                                         return_output=True)[0].split('\\n')[0])\n",
    "    downsampled_depths.append([sample_data.title, count_downsample, count_spike_downsample])\n",
    "    \n",
    "    print(f'Downsampling: {sample_data.title} done')\n",
    "    \n",
    "downsample_DF = pd.DataFrame(downsampled_depths, columns = ['Sample_Name','Downsampled_read_counts',\n",
    "                                                           'Downsampled_spike_read_counts'])\n",
    "downsample_DF.to_csv(f'{downsample_dir}/downsampled_depths.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf8ff7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that spike read proportion was preserved\n",
    "downsampled_depths = pd.read_csv(f'{downsample_dir}/downsampled_depths.csv')\n",
    "previous_spike_reads = pd.read_csv(f'{spike_R2_alignments_dir}/R2_read_counts.csv')\n",
    "previous_experimental_reads = pd.read_csv(f'{R2_alignments_dir}/R2_read_counts.csv')\n",
    "\n",
    "downsampled_depths['Total starting R2 reads'] = previous_spike_reads['R2_read_counts'] + \\\n",
    "                                    previous_experimental_reads['R2_read_counts']\n",
    "downsampled_depths['Total new R2 reads'] = downsampled_depths['Downsampled_read_counts'] + \\\n",
    "                                           downsampled_depths['Downsampled_spike_read_counts']\n",
    "\n",
    "downsampled_depths['Previous spike proportion'] = previous_spike_reads['R2_read_counts'] / downsampled_depths['Total R2 reads']\n",
    "downsampled_depths['New spike proportion'] = downsampled_depths['Downsampled_spike_read_counts'] / downsampled_depths['Total new R2 reads']\n",
    "downsampled_depths.to_csv(f'{downsample_dir}/spike_proportion_check.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e447528f",
   "metadata": {},
   "source": [
    "## Generate single-bp resolution count data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9aa115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate single-bp coverage: both ends and total coverage\n",
    "\n",
    "# Set print_output = True to see command-line output\n",
    "# Set return_output = False if assigning commmand-line output to a variable\n",
    "print_output = True\n",
    "return_output = False\n",
    "\n",
    "# Generate pull out ends of downsampled R2 reads\n",
    "for row in sample_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # prepare input and output titles\n",
    "    bam_input = f'{downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    end_outputs = [\n",
    "        f'{ends_dir}/{sample_data.title}_ends_plus.txt',\n",
    "        f'{ends_dir}/{sample_data.title}_ends_minus.txt']\n",
    "    # commands\n",
    "    command_plus = f'bedtools genomecov -ibam {bam_input} -d -strand + -5 > {end_outputs[0]}'\n",
    "    command_minus = f'bedtools genomecov -ibam {bam_input} -d -strand - -5 > {end_outputs[1]}'\n",
    "    output_plus = quickshell(command_plus, print_output = print_output, return_output = return_output)\n",
    "    output_minus = quickshell(command_minus, print_output = print_output, return_output = return_output)\n",
    "    print(f'Single-bp end coverage: {sample_data.title} done')\n",
    "\n",
    "# Generate single-bp coverage of downsampled R2 reads\n",
    "for row in sample_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # prepare input and output titles\n",
    "    bam_input = f'{downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    end_outputs = [\n",
    "        f'{coverage_dir}/{sample_data.title}_coverage_plus.txt',\n",
    "        f'{coverage_dir}/{sample_data.title}_coverage_minus.txt']\n",
    "    # commands\n",
    "    command_plus = f'bedtools genomecov -ibam {bam_input} -d -strand + > {end_outputs[0]}'\n",
    "    command_minus = f'bedtools genomecov -ibam {bam_input} -d -strand - > {end_outputs[1]}'\n",
    "    output_plus = quickshell(command_plus, print_output = print_output, return_output = return_output)\n",
    "    output_minus = quickshell(command_minus, print_output = print_output, return_output = return_output)\n",
    "    print(f'Single-bp full coverage: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63218434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate single-bp coverage of spike sample: both ends and total coverage\n",
    "\n",
    "# Set print_output = True to see command-line output\n",
    "# Set return_output = False if assigning commmand-line output to a variable\n",
    "print_output = True\n",
    "return_output = False\n",
    "\n",
    "# Generate pull out ends of downsampled R2 reads\n",
    "for row in sample_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # prepare input and output titles\n",
    "    bam_input = f'{spike_downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    end_outputs = [\n",
    "        f'{spike_ends_dir}/{sample_data.title}_ends_plus.txt',\n",
    "        f'{spike_ends_dir}/{sample_data.title}_ends_minus.txt']\n",
    "    # commands\n",
    "    command_plus = f'bedtools genomecov -ibam {bam_input} -d -strand + -5 > {end_outputs[0]}'\n",
    "    command_minus = f'bedtools genomecov -ibam {bam_input} -d -strand - -5 > {end_outputs[1]}'\n",
    "    output_plus = quickshell(command_plus, print_output = print_output, return_output = return_output)\n",
    "    output_minus = quickshell(command_minus, print_output = print_output, return_output = return_output)\n",
    "    print(f'Single-bp spike end coverage: {sample_data.title} done')\n",
    "\n",
    "# Generate single-bp coverage of downsampled R2 reads\n",
    "for row in sample_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    # prepare input and output titles\n",
    "    bam_input = f'{spike_downsample_dir}/{sample_data.title}_downsample.bam'\n",
    "    end_outputs = [\n",
    "        f'{spike_coverage_dir}/{sample_data.title}_coverage_plus.txt',\n",
    "        f'{spike_coverage_dir}/{sample_data.title}_coverage_minus.txt']\n",
    "    # commands\n",
    "    command_plus = f'bedtools genomecov -ibam {bam_input} -d -strand + > {end_outputs[0]}'\n",
    "    command_minus = f'bedtools genomecov -ibam {bam_input} -d -strand - > {end_outputs[1]}'\n",
    "    output_plus = quickshell(command_plus, print_output = print_output, return_output = return_output)\n",
    "    output_minus = quickshell(command_minus, print_output = print_output, return_output = return_output)\n",
    "    print(f'Single-bp spike full coverage: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2abc37e",
   "metadata": {},
   "source": [
    "## Generate bigWig inputs for enriched end calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bc3e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set print_output = True to see command-line output\n",
    "# Set return_output = False if assigning commmand-line output to a variable\n",
    "print_output = False\n",
    "return_output = False\n",
    "\n",
    "Mtb_region = 'Eco_Mtb:4641653:9053361'\n",
    "\n",
    "# Generate bigWig files for peak calling\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    \n",
    "    bigwig_plus_command = f'bamCoverage --bam {downsample_dir}/{sample_data.title}_downsample.bam ' + \\\n",
    "                          f'--numberOfProcessors {processors} --region {ends_region} ' + \\\n",
    "                          f'--skipNonCoveredRegions --skipNAs ' + \\\n",
    "                          f'--outFileName {bigWig_dir}/{sample_data.title}_plus.bw --outFileFormat bigwig ' + \\\n",
    "                          f'--binSize 1 --filterRNAstrand forward --Offset 1' \n",
    "    bigwig_minus_command = f'bamCoverage --bam {downsample_dir}/{sample_data.title}_downsample.bam ' + \\\n",
    "                           f'--numberOfProcessors {processors} --region {ends_region} '+ \\\n",
    "                           f'--skipNonCoveredRegions --skipNAs ' + \\\n",
    "                           f'--outFileName {bigWig_dir}/{sample_data.title}_minus.bw --outFileFormat bigwig ' + \\\n",
    "                           f'--binSize 1 --filterRNAstrand reverse --Offset 1' \n",
    " \n",
    "    bigwig_plus = quickshell(bigwig_plus_command, print_output = print_output, return_output = return_output)\n",
    "    print(f'bigWig generation (calling input): {sample_data.title} plus done')\n",
    "    bigwig_minus = quickshell(bigwig_minus_command, print_output = print_output, return_output = return_output)\n",
    "    print(f'bigWig generation (calling input): {sample_data.title} minus done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ef54ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike\n",
    "# Set print_output = True to see command-line output\n",
    "# Set return_output = False if assigning commmand-line output to a variable\n",
    "print_output = False\n",
    "return_output = False\n",
    "\n",
    "Eco_region = 'Eco_Mtb:1-4641652'\n",
    "\n",
    "# Generate bigWig files for peak calling\n",
    "for row in inline_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    \n",
    "    bigwig_plus_command = f'bamCoverage --bam {spike_downsample_dir}/{sample_data.title}_downsample.bam ' + \\\n",
    "                          f'--numberOfProcessors {processors} --region {ends_region} ' + \\\n",
    "                          f'--skipNonCoveredRegions --skipNAs ' + \\\n",
    "                          f'--outFileName {spike_bigWig_dir}/{sample_data.title}_plus.bw --outFileFormat bigwig ' + \\\n",
    "                          f'--binSize 1 --filterRNAstrand forward --Offset 1' \n",
    "    bigwig_minus_command = f'bamCoverage --bam {spike_downsample_dir}/{sample_data.title}_downsample.bam ' + \\\n",
    "                           f'--numberOfProcessors {processors} --region {ends_region} '+ \\\n",
    "                           f'--skipNonCoveredRegions --skipNAs ' + \\\n",
    "                           f'--outFileName {spike_bigWig_dir}/{sample_data.title}_minus.bw --outFileFormat bigwig ' + \\\n",
    "                           f'--binSize 1 --filterRNAstrand reverse --Offset 1' \n",
    " \n",
    "    bigwig_plus = quickshell(bigwig_plus_command, print_output = print_output, return_output = return_output)\n",
    "    print(f'spike bigWig generation (calling input): {sample_data.title} plus done')\n",
    "    bigwig_minus = quickshell(bigwig_minus_command, print_output = print_output, return_output = return_output)\n",
    "    print(f'spike bigWig generation (calling input): {sample_data.title} minus done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7b41b3",
   "metadata": {},
   "source": [
    "## Call enriched ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32803861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental samples\n",
    "\n",
    "for row in sample_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    end_enrich_command = f'python3 NETseq_pause_calling.py Genomewide ' + \\\n",
    "                         f'{bigWig_dir}/{sample_data.title}_plus.bw ' + \\\n",
    "                         f'{bigWig_dir}/{sample_data.title}_minus.bw {window} --p {processors} ' + \\\n",
    "                         f'--method bootstrap --filter cov --cov_cutoff {cov_cutoff} ' + \\\n",
    "                         f'--alpha {alpha} > {bootstrap_calls_dir}/{sample_data.title}_calls_alpha{alpha}.txt'\n",
    "    end_enrich = quickshell(end_enrich_command, print_output = False, return_output = False)\n",
    "    print(f'Bootstrap calling: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d22e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spike samples\n",
    "\n",
    "for row in sample_table.iterrows():\n",
    "    _, sample_data = row\n",
    "    end_enrich_command = f'python3 NETseq_pause_calling.py Genomewide ' + \\\n",
    "                         f'{spike_bigWig_dir}/{sample_data.title}_plus.bw ' + \\\n",
    "                         f'{spike_bigWig_dir}/{sample_data.title}_minus.bw {window} --p {processors} ' + \\\n",
    "                         f'--method bootstrap --filter cov --cov_cutoff {cov_cutoff} ' + \\\n",
    "                         f'--alpha {alpha} > {spike_bootstrap_calls_dir}/{sample_data.title}_calls_alpha{alpha}.txt'\n",
    "    end_enrich = quickshell(end_enrich_command, print_output = False, return_output = False)\n",
    "    print(f'Spike bootstrap calling: {sample_data.title} done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8375ec",
   "metadata": {},
   "source": [
    "# Extract spike consensus calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e9302",
   "metadata": {},
   "source": [
    "Use CellFreeGenomics_thresholdSelection to systematically test different count thresholds for experimental data.\n",
    "\n",
    "Usually, spike data only comprise 5-10% of the sample and are just used for size factor estimation,\n",
    "so the lower count threshold of 2 works well to get enough peaks for size factor estimation in DESeq2.\n",
    "\n",
    "As a result, spike consensus calls can be easily generated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d0150b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_peaks(condition, replicates, count_cutoff):\n",
    "    \"\"\"\n",
    "    Generates a consensus dataframe of called peaks from a list of replicates.\n",
    "    condition is a string containing the name of the experimental condition, e.g. NusG.\n",
    "    replicates: number of replicates.\n",
    "    count_cutoff: >= cutoff value in 'ends' column.\n",
    "    \"\"\"\n",
    "    # Initialize empty lists\n",
    "    plus_replicate_DF_list = []\n",
    "    minus_replicate_DF_list = []\n",
    "    plus_ends_list = []\n",
    "    minus_ends_list = []\n",
    "    \n",
    "    replicate_numbers_DF = sample_table.loc[sample_table['condition'] == condition,]\n",
    "    replicate_numbers_DF.reset_index(inplace = True)\n",
    "    replicate_numbers = replicate_numbers_DF['replicate']\n",
    "    \n",
    "    # For each replicate:\n",
    "    for i in range(replicates):\n",
    "        \n",
    "        # Read in the called peaks for each replicate\n",
    "        next_DF = pd.read_table(f'{spike_bootstrap_calls_dir}/{condition}{replicate_numbers[i]}' + \\\n",
    "                                f'_calls_alpha{alpha}.txt')\n",
    "        # Add the replicate as a column\n",
    "        next_DF['replicate'] = replicate_numbers[i]\n",
    "        \n",
    "        # Filter for ends that have a certain number of counts\n",
    "        next_DF_highCov = next_DF.loc[next_DF['count'] >= 2,]\n",
    "        \n",
    "        # Adjust p-values again after applying coverage filter\n",
    "        _, qvals = fdrcorrection(next_DF_highCov['pvalue'], method = \"i\")\n",
    "        next_DF_highCov['qvalue_updated'] = qvals\n",
    "        \n",
    "        # Filter for significant peaks (q-val = post-multiple hypothesis testing correction)\n",
    "        next_DF_sig = next_DF_highCov.loc[next_DF_highCov['qvalue_updated'] <= sig_cutoff,]\n",
    "        \n",
    "        # Separate + and - coordinates\n",
    "        next_DF_sig_plus = next_DF_sig.loc[next_DF_sig['strand'] == '+',]\n",
    "        next_DF_sig_minus = next_DF_sig.loc[next_DF_sig['strand'] == '-',]\n",
    "        \n",
    "        # Append plus and minus coordinates to empty lists\n",
    "        plus_replicate_DF_list.append(next_DF_sig_plus)\n",
    "        minus_replicate_DF_list.append(next_DF_sig_minus)\n",
    "        plus_ends_list.append(set(next_DF_sig_plus['end']))\n",
    "        minus_ends_list.append(set(next_DF_sig_minus['end']))\n",
    "        \n",
    "    # Put all individual replicates per condition into one large DataFrame\n",
    "    plus_replicate_DFs = pd.concat(plus_replicate_DF_list)\n",
    "    minus_replicate_DFs = pd.concat(minus_replicate_DF_list)\n",
    "    \n",
    "    # Only include enriched ends that were called in all replicates\n",
    "    plus_ends_intersection = set.intersection(*plus_ends_list)\n",
    "    minus_ends_intersection = set.intersection(*minus_ends_list)\n",
    "    plus_replicate_DF_allReps = plus_replicate_DFs.loc[plus_replicate_DFs['end'].isin(plus_ends_intersection),]\n",
    "    minus_replicate_DF_allReps = minus_replicate_DFs.loc[minus_replicate_DFs['end'].isin(minus_ends_intersection),]\n",
    "    \n",
    "    # Combine + and - DFs\n",
    "    replicate_DF_allReps = pd.concat([plus_replicate_DF_allReps, minus_replicate_DF_allReps])\n",
    "    \n",
    "    replicate_DF_allReps['condition'] = condition\n",
    "    \n",
    "    # Write out final dataframe\n",
    "    replicate_DF_allReps.to_csv(f'{spike_bootstrap_calls_dir}/' + \\\n",
    "                    f'spike_{condition}_consensus_calls_{replicates}reps.csv')\n",
    "    print(f'Generating consensus spike peaks: {condition} done')\n",
    "#    return replicate_DF_allReps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85ee1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_table['condition'] = sample_table['title'].str[:-1]\n",
    "sample_table['replicate'] = sample_table['title'].str[-1:]\n",
    "condition_list = sample_table['condition'].unique()\n",
    "\n",
    "for condition in condition_list:\n",
    "    consensus_peaks(condition, replicates, cov_cutoff)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
