{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b4885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import upsetplot\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from scipy.signal import find_peaks\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from statistics import mean, stdev, median\n",
    "import math\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from Bio import SeqIO\n",
    "import csv\n",
    "from toolz import interleave\n",
    "import re\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615c1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by Peter Culviner, PhD to enable command-line access through Jupyter\n",
    "def quickshell(command, print_output=True, output_path=None, return_output=False):\n",
    "    process_output = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    stdout = process_output.stdout.decode('utf-8')\n",
    "    stderr = process_output.stderr.decode('utf-8')\n",
    "    output_string = f'STDOUT:\\n{stdout}\\nSTDERR:\\n{stderr}\\n'\n",
    "    if print_output:\n",
    "        print('$ ' + command)\n",
    "        print(output_string)\n",
    "    if output_path is not None:\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.write(output_string)\n",
    "    if return_output:\n",
    "        return stdout, stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d452ea8",
   "metadata": {},
   "source": [
    "# Initializing inputs and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b48069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main variables to adjust\n",
    "bootstrap_alpha = 1\n",
    "bootstrap_sig_cutoff = 0.05\n",
    "count_cutoff_range = range(10,210,10)\n",
    "replicates = 3\n",
    "DESeq2_sig_cutoff\n",
    "\n",
    "# Length of spike genome (Eco) - adjust as needed\n",
    "len_spike_genome = 4641651\n",
    "# Length of experimental genome (Mtb) - adjust as needed\n",
    "len_ref_genome = 4411709\n",
    "\n",
    "# Pairwise comparison\n",
    "condition_subset = ['noCRP','CRP']\n",
    "blacklist_sample = 'core'\n",
    "\n",
    "condition_subset = ['noTF', 'NusA', 'NusG', 'NusA_NusG']\n",
    "blacklist_sample = 'gDNA'\n",
    "\n",
    "condition_string = \"_\".join(condition_subset)\n",
    "\n",
    "# Executable path for the MEME algorithm (depends on where you have it installed)\n",
    "MEME_path = '/usr/local/bin/meme/bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06770a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Motif information\n",
    "\n",
    "# 5enrich experiment: \n",
    "# QC (quality control) motif: -10 element\n",
    "# DE (differential expression) motif: CRP motif\n",
    "outer_up = 100\n",
    "outer_down = 30\n",
    "QCmotifUp = 16\n",
    "QCmotifDown = 4\n",
    "QCmotifName = 'min10'\n",
    "QCmotifRegex = 'A...U'\n",
    "DEmotifUp = 100\n",
    "DEmotifDown = -30\n",
    "DEmotifName = 'min100plus30'\n",
    "DEmotifRegex = 'GUG........CAC'\n",
    "\n",
    "# 3enrich experiment: \n",
    "# QC motif: U-tract\n",
    "# TF motif: none\n",
    "outer_up = 50\n",
    "outer_down = 10\n",
    "QCmotifUp = -9\n",
    "QCmotifDown = -4\n",
    "QCmotifName = 'utract'\n",
    "QCmotifRegex = 'UUUU'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2c0f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "main_path = '5enrich_CRP'\n",
    "#main_path = '3enrich_NusAG'\n",
    "gDNA_path = 'gDNA'\n",
    "\n",
    "# Directories from readPrep\n",
    "readPrep_dir = f'{main_path}/readPrep'\n",
    "R2_alignments_dir = f'{readPrep_dir}/R2_alignments'\n",
    "\n",
    "# Directories from identifyEnrichedEnds\n",
    "identifyEnds_dir = f'{main_path}/identifyEnrichedEnds'\n",
    "bigWig_dir = f'{identifyEnds_dir}/bigWigs'\n",
    "bootstrap_calls_dir = f'{identifyEnds_dir}/bootstrap_calls'\n",
    "ends_dir = f'{identifyEnds_dir}/ends'\n",
    "coverage_dir = f'{identifyEnds_dir}/coverage'\n",
    "spike_ends_dir = f'{identifyEnds_dir}/ends_spike'\n",
    "spike_coverage_dir = f'{identifyEnds_dir}/cov_spike'\n",
    "spike_bootstrap_calls_dir = f'{identifyEnds_dir}/bootstrap_calls_spike'\n",
    "\n",
    "# Outputs\n",
    "selectThreshold_dir = f'{main_path}/selectThreshold'\n",
    "preBL_dir = f'{selectThreshold_dir}/transcriptEnds_preBlackList'\n",
    "postBL_dir = f'{selectThreshold_dir}/transcriptEnds_postBlackList'\n",
    "MEME_outputs_dir = f'{selectThreshold_dir}/MEME_outputs'\n",
    "union_dir = f'{selectThreshold_dir}/transcriptEndUnions'\n",
    "spike_union_dir = f'{identifyEnds_dir}/spikeTranscriptEndUnions'\n",
    "DESeq2_dir = f'{selectThreshold_dir}/DESeq2'\n",
    "seqExtract_dir = f'{selectThreshold_dir}/flankingSeqs'\n",
    "motif_compilation_dir = f'{selectThreshold_dir}/allMotifResults'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c97f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir $selectThreshold_dir\n",
    "!mkdir $preBL_dir\n",
    "!mkdir $postBL_dir\n",
    "!mkdir $MEME_outputs_dir\n",
    "!mkdir $union_dir\n",
    "!mkdir $spike_union_dir\n",
    "!mkdir $DESeq2_dir\n",
    "!mkdir $seqExtract_dir\n",
    "!mkdir $motif_compilation_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a4d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = pd.read_csv(f'{readPrep_dir}/downsampled_R2_bams/downsampled_depths.csv')\n",
    "samples = sampleDF['Sample_Name']\n",
    "sampleDF['condition'] = sampleDF['Sample_Name'].str[:-1]\n",
    "sampleDF['replicate'] = sampleDF['Sample_Name'].str[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c4588e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import genome\n",
    "genome_iterator = SeqIO.parse(f'genome_files_misc/Eco_Mtb_genome.fasta', 'fasta')\n",
    "genome = 0\n",
    "\n",
    "for seq_record in genome_iterator:\n",
    "    genome = seq_record.seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368a3b31",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a394c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_peaks(condition, replicates, count_cutoff):\n",
    "    \"\"\"\n",
    "    Generates a consensus dataframe of called peaks from a list of replicates.\n",
    "    condition is a string containing the name of the experimental condition, e.g. NusG.\n",
    "    replicates: number of replicates.\n",
    "    count_cutoff: >= cutoff value in 'ends' column.\n",
    "    \"\"\"\n",
    "    # Initialize empty lists\n",
    "    plus_replicate_DF_list = []\n",
    "    minus_replicate_DF_list = []\n",
    "    plus_ends_list = []\n",
    "    minus_ends_list = []\n",
    "    \n",
    "    replicate_numbers_DF = sampleDF.loc[sampleDF['condition'] == condition,]\n",
    "    replicate_numbers_DF.reset_index(inplace = True)\n",
    "    replicate_numbers = replicate_numbers_DF['replicate']\n",
    "    \n",
    "    # For each replicate:\n",
    "    for i in range(replicates):\n",
    "        \n",
    "        # Read in the called peaks for each replicate\n",
    "        next_DF = pd.read_table(f'{bootstrap_calls_dir}/{condition}{replicate_numbers[i]}' + \\\n",
    "                                f'_calls_alpha{bootstrap_alpha}.txt')\n",
    "        # Add the replicate as a column\n",
    "        next_DF['replicate'] = replicate_numbers[i]\n",
    "        \n",
    "        # Filter for ends that have a certain number of counts\n",
    "        next_DF_highCov = next_DF.loc[next_DF['count'] >= count_cutoff,]\n",
    "        \n",
    "        # Adjust p-values again after applying coverage filter\n",
    "        _, qvals = fdrcorrection(next_DF_highCov['pvalue'], method = \"i\")\n",
    "        next_DF_highCov['qvalue_updated'] = qvals\n",
    "        \n",
    "        # Filter for significant peaks (q-val = post-multiple hypothesis testing correction)\n",
    "        next_DF_sig = next_DF_highCov.loc[next_DF_highCov['qvalue_updated'] <= bootstrap_sig_cutoff,]\n",
    "        \n",
    "        # Separate + and - coordinates\n",
    "        next_DF_sig_plus = next_DF_sig.loc[next_DF_sig['strand'] == '+',]\n",
    "        next_DF_sig_minus = next_DF_sig.loc[next_DF_sig['strand'] == '-',]\n",
    "        \n",
    "        # Append plus and minus coordinates to empty lists\n",
    "        plus_replicate_DF_list.append(next_DF_sig_plus)\n",
    "        minus_replicate_DF_list.append(next_DF_sig_minus)\n",
    "        plus_ends_list.append(set(next_DF_sig_plus['end']))\n",
    "        minus_ends_list.append(set(next_DF_sig_minus['end']))\n",
    "        \n",
    "    # Put all individual replicates per condition into one large DataFrame\n",
    "    plus_replicate_DFs = pd.concat(plus_replicate_DF_list)\n",
    "    minus_replicate_DFs = pd.concat(minus_replicate_DF_list)\n",
    "    \n",
    "    # Only include enriched ends that were called in all replicates\n",
    "    plus_ends_intersection = set.intersection(*plus_ends_list)\n",
    "    minus_ends_intersection = set.intersection(*minus_ends_list)\n",
    "    plus_replicate_DF_allReps = plus_replicate_DFs.loc[plus_replicate_DFs['end'].isin(plus_ends_intersection),]\n",
    "    minus_replicate_DF_allReps = minus_replicate_DFs.loc[minus_replicate_DFs['end'].isin(minus_ends_intersection),]\n",
    "    \n",
    "    # Combine + and - DFs\n",
    "    replicate_DF_allReps = pd.concat([plus_replicate_DF_allReps, minus_replicate_DF_allReps])\n",
    "    \n",
    "    replicate_DF_allReps['condition'] = condition\n",
    "    \n",
    "    # Write out final dataframe\n",
    "    replicate_DF_allReps.to_csv(f'{preBL_dir}/' + \\\n",
    "                    f'preBlackList_{condition}_consensus_calls_{replicates}reps_{count_cutoff}counts.csv')\n",
    "    print(f'Generating consensus peaks: {condition} done')\n",
    "    return replicate_DF_allReps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2921ba6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensus_peaks_gDNA(replicates, count_cutoff):\n",
    "    \"\"\"\n",
    "    Generates a consensus dataframe for genomic DNA peaks.\n",
    "    replicates: number of replicates.\n",
    "    count_cutoff: >= cutoff value in 'ends' column.\n",
    "    \"\"\"\n",
    "    # Initialize empty lists\n",
    "    plus_replicate_DF_list = []\n",
    "    minus_replicate_DF_list = []\n",
    "    plus_ends_list = []\n",
    "    minus_ends_list = []\n",
    "    \n",
    "    # For each replicate:\n",
    "    for i in range(replicates):\n",
    "        \n",
    "        # Read in the called peaks for each replicate\n",
    "        next_DF = pd.read_table(f'{gDNA_dir}/identifyEnrichedEnds/bootstrap_calls/gDNA{replicate_numbers[i]}' + \\\n",
    "                                f'_calls_alpha{bootstrap_alpha}.txt')\n",
    "        # Add the replicate as a column\n",
    "        next_DF['replicate'] = replicate_numbers[i]\n",
    "        \n",
    "        # Filter for ends that have a certain number of counts\n",
    "        next_DF_highCov = next_DF.loc[next_DF['count'] >= count_cutoff,]\n",
    "        \n",
    "        # Adjust p-values again after applying coverage filter\n",
    "        _, qvals = fdrcorrection(next_DF_highCov['pvalue'], method = \"i\")\n",
    "        next_DF_highCov['qvalue_updated'] = qvals\n",
    "        \n",
    "        # Filter for significant peaks (q-val = post-multiple hypothesis testing correction)\n",
    "        next_DF_sig = next_DF_highCov.loc[next_DF_highCov['qvalue_updated'] <= bootstrap_sig_cutoff,]\n",
    "        \n",
    "        # Separate + and - coordinates\n",
    "        next_DF_sig_plus = next_DF_sig.loc[next_DF_sig['strand'] == '+',]\n",
    "        next_DF_sig_minus = next_DF_sig.loc[next_DF_sig['strand'] == '-',]\n",
    "        \n",
    "        # Append plus and minus coordinates to empty lists\n",
    "        plus_replicate_DF_list.append(next_DF_sig_plus)\n",
    "        minus_replicate_DF_list.append(next_DF_sig_minus)\n",
    "        plus_ends_list.append(set(next_DF_sig_plus['end']))\n",
    "        minus_ends_list.append(set(next_DF_sig_minus['end']))\n",
    "        \n",
    "    # Put all individual replicates per condition into one large DataFrame\n",
    "    plus_replicate_DFs = pd.concat(plus_replicate_DF_list)\n",
    "    minus_replicate_DFs = pd.concat(minus_replicate_DF_list)\n",
    "    \n",
    "    # Only include enriched ends that were called in all replicates\n",
    "    plus_ends_intersection = set.intersection(*plus_ends_list)\n",
    "    minus_ends_intersection = set.intersection(*minus_ends_list)\n",
    "    plus_replicate_DF_allReps = plus_replicate_DFs.loc[plus_replicate_DFs['end'].isin(plus_ends_intersection),]\n",
    "    minus_replicate_DF_allReps = minus_replicate_DFs.loc[minus_replicate_DFs['end'].isin(minus_ends_intersection),]\n",
    "    \n",
    "    # Combine + and - DFs\n",
    "    replicate_DF_allReps = pd.concat([plus_replicate_DF_allReps, minus_replicate_DF_allReps])\n",
    "    \n",
    "    replicate_DF_allReps['condition'] = 'gDNA'\n",
    "    \n",
    "    # Write out final dataframe\n",
    "    replicate_DF_allReps.to_csv(f'{gDNA_dir}/selectThreshold/enrichedEnds/' + \\\n",
    "                    f'gDNA_consensus_calls_{replicates}reps_{count_cutoff}counts.csv')\n",
    "    print(f'Generating consensus peaks: gDNA done')\n",
    "    return replicate_DF_allReps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cb2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_blacklist_coordinates(coordinate_DF, blacklist_DF, column):\n",
    "    '''\n",
    "    Removes coordinates from coordinate_DF that are present in blacklist_DF.\n",
    "    \n",
    "    Preconditions: inputs must be Pandas DataFrames with 'end' columns.\n",
    "    '''\n",
    "    blacklist_coords = blacklist_DF[f'{column}']\n",
    "    coordinate_DF_noBlackList = coordinate_DF.loc[~coordinate_DF[f'{column}'].isin(blacklist_coords),]\n",
    "    return coordinate_DF_noBlackList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab321f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_coordinate_environment(genome_seq, coord_DF, upstream_end, downstream_end, samples, count_cutoff):\n",
    "    \n",
    "    coord_DF['IndexingCoordinate'] = coord_DF['coordinate']\n",
    "    double_genome = genome_seq + genome_seq\n",
    "    \n",
    "    # Update the indexing coordinate for instances where the indexing requires wrapping around the genome\n",
    "    coord_DF.loc[(coord_DF['coordinate'] < upstream_end) & (coord_DF['strand'] == '-'),'IndexingCoordinate'] = coord_DF.loc[(coord_DF['coordinate'] < upstream_end) & (coord_DF['strand'] == '-'),\"coordinate\"] + len(genome_seq)\n",
    "    coord_DF.loc[(coord_DF['coordinate'] < downstream_end) & (coord_DF['strand'] == '+'),'IndexingCoordinate'] = coord_DF.loc[(coord_DF['coordinate'] < downstream_end) & (coord_DF['strand'] == '+'),'coordinate'] + len(genome_seq)\n",
    "    \n",
    "    plus_DF = coord_DF.loc[coord_DF['strand'] == \"+\",]\n",
    "    minus_DF = coord_DF.loc[coord_DF['strand'] == \"-\",]\n",
    "  \n",
    "    # Initializes the ranges of the genome to extract\n",
    "    plus_DF['start'] = plus_DF['IndexingCoordinate'] - upstream_end\n",
    "    plus_DF['end'] = plus_DF['IndexingCoordinate'] + downstream_end\n",
    "    plus_DF['IndexCoordMin1'] = plus_DF['IndexingCoordinate'] - 1\n",
    "    plus_DF['IndexCoordPlus1'] = plus_DF['IndexingCoordinate'] + 1\n",
    "    plus_DF['Nontemplate_strand'] = ''\n",
    "    plus_DF.reset_index(inplace = True)\n",
    "                                                    \n",
    "    for i in range(len(plus_DF.index)):\n",
    "        # Breaks up the extracted region to capitalize the TSS\n",
    "        # Promoter is the area directly upstream of the coordinate\n",
    "        plus_upstream = double_genome[int(plus_DF['start'][i]):int(plus_DF['IndexingCoordinate'][i])].lower()\n",
    "        plus_coordinate = double_genome[int(plus_DF['IndexingCoordinate'][i]):int(plus_DF['IndexCoordPlus1'][i])]\n",
    "        plus_downstream = double_genome[int(plus_DF['IndexCoordPlus1'][i]):int(plus_DF['end'][i])].lower()\n",
    "        plus_DF['Nontemplate_strand'][i] = plus_upstream + plus_coordinate + plus_downstream\n",
    "  \n",
    "    # Initializes the ranges of the genome to extract\n",
    "    minus_DF['start'] = minus_DF['IndexingCoordinate'] - (downstream_end-1)\n",
    "    minus_DF['end'] = minus_DF['IndexingCoordinate'] + (upstream_end+1)\n",
    "    minus_DF['IndexCoordMin1'] = minus_DF['IndexingCoordinate'] - 1\n",
    "    minus_DF['IndexCoordPlus1'] = minus_DF['IndexingCoordinate'] + 1\n",
    "    minus_DF['Nontemplate_strand'] = ''\n",
    "    \n",
    "    minus_DF.reset_index(inplace = True)\n",
    "  \n",
    "    for i in range(len(minus_DF.index)):\n",
    "                                                    \n",
    "        # For - strand regions turns the extracted region into a DNA sequence and takes the reverse complement\n",
    "        # (converts the + strand to the - strand) to get the non-template strand\n",
    "        minus_extract_all = double_genome[int(minus_DF['start'][i]):int(minus_DF['end'][i])]\n",
    "        minus_NTstrand = minus_extract_all.reverse_complement()\n",
    "        \n",
    "        # Breaks up the extracted region to capitalize the TSS\n",
    "        # Promoter is the area directly upstream of the coordinate\n",
    "        minus_upstream = minus_NTstrand[:upstream_end].lower()\n",
    "        minus_coordinate = minus_NTstrand[upstream_end:upstream_end+1]\n",
    "        minus_downstream = minus_NTstrand[upstream_end+1:].lower()\n",
    "        minus_DF['Nontemplate_strand'][i] = minus_upstream + minus_coordinate + minus_downstream\n",
    "\n",
    "    complete_DF = pd.concat([plus_DF, minus_DF])\n",
    "    complete_DF_ordered = complete_DF.sort_values(by = 'coordinate')\n",
    "    complete_DF_ordered.to_csv(f'{seqExtract_dir}/{samples}_{count_cutoff}counts_seqExtract.csv')\n",
    "    return complete_DF_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80c8ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meme_formatting(DF, region_column, sample_name, MEME_dir_counts):\n",
    "    \"\"\"\n",
    "    Writing out FASTA files in the MEME format.\n",
    "    DF = DataFrame.\n",
    "    Region_column = the column containing sequences of interest. For MEME, needs to be >8bp.\n",
    "    MEME_dir_counts = string with local directory to output files.\n",
    "    \"\"\"\n",
    "    DF['fasta_ID'] = '>' + DF['coordinate'].astype('str') + DF['strand']\n",
    "    fasta_out = DF['fasta_ID'] + '\\n' + DF[f'{region_column}']\n",
    "    fasta_out_name = f'{MEME_dir_counts}/{sample_name}.fasta'\n",
    "    fasta_out.to_csv(fasta_out_name, index=False, quoting = csv.QUOTE_NONE, escapechar = '(', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77d7e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meme_validation(exp_DF, control_DF, outer_up, outer_down, motif_up, motif_down,\n",
    "                    condition, count_cutoff, meme_algorithm, motif, MEME_dir_counts):\n",
    "    \"\"\"\n",
    "    Extract sequences & carry out MEME validation of desired motif.\n",
    "    \n",
    "    exp_DF: a dataframe with enriched coordinates. Requires a 'coordinate' and 'strand' column.\n",
    "    control_DF: a dataframe with control coordinates. \n",
    "        For bootstrap call threshold validation, likely to be a set of random coordinates.\n",
    "        For differential expression validation, likely to be the union of coordinates called in the condition set.\n",
    "    outer_up: an integer with the amount upstream of the coordinate to extract.\n",
    "    outer_down: an integer with the amount upstream of the coordinate to extract\n",
    "        (negative numbers to go upstream of the coordinate.)\n",
    "    motif_up: upstream bound of the motif region (extracted via outer_up - motif_up)\n",
    "    motif_down: downstream bound of the motif region. (extracted via outer_up - motif_down)\n",
    "    condition: a string with the sample name (can be one condition, or multiple conditions joined from a list).\n",
    "    count_cutoff: integer with the end count threshold for bootstrap calling.\n",
    "    meme_algorithm: XSTREME, MEME or STREME. XSTREME encompasses MEME and STREME, so it is recommended.\n",
    "    motif: string describing the motif being examined for output naming (e.g. min10, CRP).\n",
    "    MEME_dir_counts: string with path to local directory for specific MEME instance.\n",
    "    \"\"\"\n",
    "    # Extract sequence environment\n",
    "    extract_coordinate_environment(genome, exp_DF, outer_up, outer_down, condition, count_cutoff)\n",
    "    extract_coordinate_environment(genome, control_DF, outer_up, outer_down, f'{condition}_control', count_cutoff)\n",
    "    # Have to re-read DFs in for correct sequence formatting\n",
    "    exp_seqDF = pd.read_csv(f'{seqExtract_dir}/{condition}_{count_cutoff}counts_seqExtract.csv')\n",
    "    control_seqDF = pd.read_csv(f'{seqExtract_dir}/{condition}_control_{count_cutoff}counts_seqExtract.csv')\n",
    "    \n",
    "    exp_seqDF['motif_region'] = exp_seqDF['Nontemplate_strand'].str[(outer_up-motif_up):(outer_up-motif_down)]\n",
    "    control_seqDF['motif_region'] = control_seqDF['Nontemplate_strand'].str[(outer_up-motif_up):(outer_up-motif_down)]\n",
    "\n",
    "    # Write out FASTA files for MEME input             \n",
    "    meme_formatting(exp_seqDF, 'motif_region', f'{condition}_{motif}_{count_cutoff}counts', MEME_dir_counts)\n",
    "    meme_formatting(control_seqDF, 'motif_region', f'{condition}_{motif}_control_{count_cutoff}counts', MEME_dir_counts)\n",
    "    replace_command = f'sed -i s/\\(//g {MEME_dir_counts}/*.fasta'\n",
    "    replace = quickshell(replace_command, print_output = False, return_output = False)\n",
    "    \n",
    "    # Call MEME from local installation\n",
    "    if meme_algorithm == \"meme\":\n",
    "        meme_call = f'{MEME_path}/meme {MEME_dir_counts}/{condition}_{motif}_{count_cutoff}counts.fasta ' + \\\n",
    "                    f'-neg {MEME_dir_counts}/{condition}_{motif}_control_{count_cutoff}counts.fasta ' + \\\n",
    "                    f'-objfun de -oc {MEME_dir_counts}/{condition}_{count_cutoff}counts_{motif}_meme -evt 0.05'\n",
    "        quickshell(meme_call, print_output = True)\n",
    "        \n",
    "    elif meme_algorithm == \"streme\":\n",
    "        streme_call = f'{MEME_path}/streme --p {MEME_dir_counts}/{condition}_{motif}_{count_cutoff}counts.fasta ' + \\\n",
    "                    f'--n {MEME_dir_counts}/{condition}_{motif}_control_{count_cutoff}counts.fasta ' + \\\n",
    "                    f'--objfun de --oc {MEME_dir_counts}/{condition}_{count_cutoff}counts_{motif}_streme --rna'\n",
    "        quickshell(streme_call, print_output = True)\n",
    "                \n",
    "    elif meme_algorithm == \"xstreme\":\n",
    "        xstreme_call = f'{MEME_path}/xstreme --p {MEME_dir_counts}/{condition}_{motif}_{count_cutoff}counts.fasta ' + \\\n",
    "                    f'--n {MEME_dir_counts}/{condition}_{motif}_control_{count_cutoff}counts.fasta ' + \\\n",
    "                    f'--rna --oc {MEME_dir_counts}/{condition}_{count_cutoff}counts_{motif}_xstreme'\n",
    "        quickshell(xstreme_call, print_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a650f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_singleBp_info(condition, replicates, strand):\n",
    "    \"\"\"\n",
    "    Reads in single-bp resolution information about end and coverage counts.\n",
    "    \n",
    "    Preconditions: \n",
    "    Absolute paths to the output from bedtools genomecov must be specified.\n",
    "    sample must be a string corresponding to a condition in the sequencing experiment, e.g. \"NusG\"\n",
    "    replicates must be an integer, corresponding to the number of replicates per condition.\n",
    "    strand must be a string stating \"minus\" or \"plus\".\n",
    "    \"\"\"\n",
    "    ends_DFs = []\n",
    "    cov_DFs = []\n",
    "    \n",
    "    replicate_numbers_DF = sampleDF.loc[sampleDF['condition'] == condition,]\n",
    "    replicate_numbers_DF.reset_index(inplace = True)\n",
    "    replicate_numbers = replicate_numbers_DF['replicate']\n",
    "    \n",
    "    for i in range(replicates):\n",
    "        single_bp_ends = pd.read_table(f'{ends_dir}/{condition}{replicate_numbers[i]}' + \\\n",
    "                                    f'_ends_{strand}.txt', skiprows=len_spike_genome,\n",
    "                                    header = 0, names = ['chr','coordinate','counts'])\n",
    "\n",
    "        single_bp_cov = pd.read_table(f'{coverage_dir}/{condition}{replicate_numbers[i]}' + \\\n",
    "                                    f'_coverage_{strand}.txt', skiprows=len_spike_genome,\n",
    "                                    header = 0, names = ['chr','coordinate','counts'])\n",
    "        \n",
    "        single_bp_ends['replicate'] = i+1\n",
    "        single_bp_cov['replicate'] = i+1\n",
    "        \n",
    "        ends_DFs.append(single_bp_ends)\n",
    "        cov_DFs.append(single_bp_cov)\n",
    "\n",
    "    return ends_DFs, cov_DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfedea9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_import_singleBp_info(condition, replicates, strand):\n",
    "    \"\"\"\n",
    "    Reads in single-bp resolution information about end and coverage counts.\n",
    "    \n",
    "    Preconditions: \n",
    "    Absolute paths to the output from bedtools genomecov must be specified.\n",
    "    sample must be a string corresponding to a condition in the sequencing experiment, e.g. \"NusG\"\n",
    "    replicates must be an integer, corresponding to the number of replicates per condition.\n",
    "    strand must be a string stating \"minus\" or \"plus\".\n",
    "    \"\"\"\n",
    "    ends_DFs = []\n",
    "    cov_DFs = []\n",
    "    \n",
    "    replicate_numbers_DF = sampleDF.loc[sampleDF['condition'] == condition,]\n",
    "    replicate_numbers_DF.reset_index(inplace = True)\n",
    "    replicate_numbers = replicate_numbers_DF['replicate']\n",
    "    \n",
    "    for i in range(replicates):\n",
    "        single_bp_ends = pd.read_table(f'{spike_ends_dir}/{condition}{replicate_numbers[i]}' + \\\n",
    "                                    f'_ends_{strand}.txt', skiprows=len_ref_genome,\n",
    "                                    header = 0, names = ['chr','coordinate','counts'])\n",
    "\n",
    "        single_bp_cov = pd.read_table(f'{spike_coverage_dir}/{condition}{replicate_numbers[i]}' + \\\n",
    "                                    f'_coverage_{strand}.txt', skiprows=len_ref_genome,\n",
    "                                    header = 0, names = ['chr','coordinate','counts'])\n",
    "        \n",
    "        single_bp_ends['replicate'] = i+1\n",
    "        single_bp_cov['replicate'] = i+1\n",
    "        \n",
    "        ends_DFs.append(single_bp_ends)\n",
    "        cov_DFs.append(single_bp_cov)\n",
    "\n",
    "    return ends_DFs, cov_DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32096215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_DF(condition_list, count_cutoff, multifactor):\n",
    "    \"\"\"\n",
    "    Generate one DataFrame with information for the union of all coordinates present any sample,\n",
    "    broken down by individual replicate.\n",
    "    \"\"\"\n",
    "    # Read in consensus dataframes\n",
    "    print('Reading in consensus DFs')\n",
    "    \n",
    "    consensus_DFs = []\n",
    "    coord_IDs = []\n",
    "    \n",
    "    # Create a list of all coordinate IDs from all conditions\n",
    "    for i in range(len(condition_list)):\n",
    "    \n",
    "        # Read in DF\n",
    "        condition_DF = pd.read_csv(f'{postBL_dir}/postBlackList_{condition_list[i]}_{count_cutoff}counts.csv')\n",
    "    \n",
    "        # Generate coordinate IDs\n",
    "        condition_DF['coordID'] = condition_DF['end'].astype(str) + condition_DF['strand']\n",
    "        \n",
    "        consensus_DFs.append(condition_DF)\n",
    "        coord_IDs.append(condition_DF['coordID'])\n",
    "        \n",
    "    # Union of coordinates in both num and den conditions\n",
    "    all_coords = set.union(*map(set,coord_IDs))\n",
    "    # Generated an ordered list for iteration\n",
    "    all_coords_list = list(all_coords)\n",
    "\n",
    "    # Read in single-bp resolutions\n",
    "    # Output format: lists of dataframes corresponding to the individual replicates for each condition\n",
    "    print('Reading in single-bp resolution information')\n",
    "    minus_ends = []\n",
    "    minus_cov = []\n",
    "    plus_ends = []\n",
    "    plus_cov = []\n",
    "    for i in range(len(condition_list)):\n",
    "        print(f'Single-bp res: reading in {condition_list[i]}')\n",
    "        minus_ends_DFs, minus_cov_DFs = import_singleBp_info(condition_list[i], replicates, \"minus\")\n",
    "        minus_ends.append(minus_ends_DFs)\n",
    "        minus_cov.append(minus_cov_DFs)\n",
    "        \n",
    "        plus_ends_DFs, plus_cov_DFs = import_singleBp_info(condition_list[i], replicates, \"plus\")\n",
    "        plus_ends.append(plus_ends_DFs)\n",
    "        plus_cov.append(plus_cov_DFs)\n",
    "        \n",
    "    # Initialize union dataframe\n",
    "    union_DF = pd.DataFrame(data = all_coords_list, columns = ['coordID'])\n",
    "    \n",
    "    # Initialize lists, where each sub-list inside corresponds to a replicate\n",
    "    # These will each become columns that populate union_DF to enable further calculations\n",
    "    counts_ends_allSamples_allReps = []\n",
    "    counts_cov_allSamples_allReps = []\n",
    "\n",
    "    for i in range(len(condition_list)):\n",
    "        \n",
    "        counts_ends_allSamples_allReps.append([])\n",
    "        counts_cov_allSamples_allReps.append([])\n",
    "        \n",
    "    # For each sample:\n",
    "    for i in range(len(condition_list)):\n",
    "\n",
    "        # For each replicate:\n",
    "        for j in range(replicates):\n",
    "            \n",
    "            print(f'Iterating through {condition_list[i]} replicate {j+1} to extract counts of ends and total coverage')\n",
    "            # Initialize dictionaries of coordID: count pairs\n",
    "            counts_ends_currentRep = {}\n",
    "            counts_cov_currentRep = {}\n",
    "\n",
    "            # Iterate through each coordinate\n",
    "            for k in range(len(all_coords_list)):\n",
    "            \n",
    "                coordID = all_coords_list[k]\n",
    "                strand = coordID[-1]\n",
    "                coordinate = coordID[:-1]\n",
    "                # For all - strand coordinates:\n",
    "                if strand == \"-\":\n",
    "                    # Get the counts for both ends only and total coverage\n",
    "                    ends_value = int(minus_ends[i][j].loc[minus_ends[i][j]['coordinate'] == int(coordinate),'counts'])\n",
    "                    cov_value = int(minus_cov[i][j].loc[minus_cov[i][j]['coordinate'] == int(coordinate),'counts'])\n",
    "\n",
    "                    # Add key, value pair of coordID, counts to replicate dictionary\n",
    "                    counts_ends_currentRep[coordID] = ends_value\n",
    "                    counts_cov_currentRep[coordID] = cov_value\n",
    "                \n",
    "                # For all + strand coordinates:    \n",
    "                else:\n",
    "                    # Get the counts for both ends only and total coverage\n",
    "                    ends_value = int(plus_ends[i][j].loc[plus_ends[i][j]['coordinate'] == int(coordinate),'counts'])\n",
    "                    cov_value = int(plus_cov[i][j].loc[plus_cov[i][j]['coordinate'] == int(coordinate),'counts'])\n",
    "\n",
    "                    # Add key, value pair of coordID, counts to replicate dictionary\n",
    "                    counts_ends_currentRep[coordID] = ends_value\n",
    "                    counts_cov_currentRep[coordID] = cov_value\n",
    "            \n",
    "            # Add each replicate dictionary to a list of all replicate dictionaries\n",
    "            counts_ends_allSamples_allReps[i].append(counts_ends_currentRep)\n",
    "            counts_cov_allSamples_allReps[i].append(counts_cov_currentRep)        \n",
    "    \n",
    "    print(\"Populating union DF with both end and total coverage counts from each replicate\")\n",
    "    # Add columns to Union_DF\n",
    "    # For each sample:\n",
    "    for i in range(len(counts_ends_allSamples_allReps)):\n",
    "        \n",
    "        # For each replicate:\n",
    "        for j in range(replicates):\n",
    "            column_prefix = f'{condition_list[i]}{j+1}'\n",
    "            union_DF[f'{column_prefix}_ends'] = union_DF['coordID'].map(counts_ends_allSamples_allReps[i][j])\n",
    "            union_DF[f'{column_prefix}_cov'] = union_DF['coordID'].map(counts_cov_allSamples_allReps[i][j])\n",
    "            union_DF[f'{column_prefix}_ER'] = union_DF[f'{column_prefix}_ends'] / union_DF[f'{column_prefix}_cov']\n",
    "\n",
    "    # Add in new columns\n",
    "    for i in range(len(condition_list)):\n",
    "        \n",
    "        union_DF[f'{condition_list[i]}_called'] = \"No\"\n",
    "    \n",
    "    print(\"Annotating conditions in which coordinate was called\")\n",
    "    # Add in whether or not a coordinate was called in a given condition\n",
    "    # Iterate through each coordinate\n",
    "    for i in range(len(union_DF.index)):\n",
    "        \n",
    "        coordID = union_DF['coordID'][i]\n",
    "    \n",
    "        # For each condition:\n",
    "        for j in range(len(condition_list)):\n",
    "            \n",
    "            # Check if the coordID is in the corresponding set of coordIDs for the condition:\n",
    "            if coordID in set(coord_IDs[j]):\n",
    "                union_DF[f'{condition_list[j]}_called'][i] = 'Yes'\n",
    "    \n",
    "    condition_string = \"_\".join(condition_list)\n",
    "    if multifactor == \"Yes\":\n",
    "        union_DF.to_csv(f'{union_dir}/union_DF_multifactor_{count_cutoff}counts.csv')\n",
    "    else:\n",
    "        union_DF.to_csv(f'{union_dir}/union_DF_{condition_string}_{count_cutoff}counts.csv')\n",
    "    print(' ')\n",
    "    return union_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bed0761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_union_DF(condition_list, count_cutoff):\n",
    "    \"\"\"\n",
    "    Generate one DataFrame with information for the union of all coordinates present any sample,\n",
    "    broken down by individual replicate.\n",
    "    \"\"\"\n",
    "    # Read in consensus dataframes\n",
    "    print('Reading in consensus DFs')\n",
    "    \n",
    "    consensus_DFs = []\n",
    "    coord_IDs = []\n",
    "    \n",
    "    # Create a list of all coordinate IDs from all conditions\n",
    "    for i in range(len(condition_list)):\n",
    "    \n",
    "        # Read in DF\n",
    "        condition_DF = pd.read_csv(f'{spike_bootstrap_calls_dir}/' + \\\n",
    "                                   f'spike_{condition_list[i]}_consensus_calls_' + \\\n",
    "                                   f'{replicates}reps_2counts.csv')\n",
    "    \n",
    "        # Generate coordinate IDs\n",
    "        condition_DF['coordID'] = condition_DF['end'].astype(str) + condition_DF['strand']\n",
    "        \n",
    "        consensus_DFs.append(condition_DF)\n",
    "        coord_IDs.append(condition_DF['coordID'])\n",
    "        \n",
    "    # Union of coordinates in both num and den conditions\n",
    "    all_coords = set.union(*map(set,coord_IDs))\n",
    "    # Generated an ordered list for iteration\n",
    "    all_coords_list = list(all_coords)\n",
    "\n",
    "    # Read in single-bp resolutions\n",
    "    # Output format: lists of dataframes corresponding to the individual replicates for each condition\n",
    "    print('Reading in single-bp resolution information')\n",
    "    minus_ends = []\n",
    "    minus_cov = []\n",
    "    plus_ends = []\n",
    "    plus_cov = []\n",
    "    for i in range(len(condition_list)):\n",
    "        print(f'Single-bp res: reading in {condition_list[i]}')\n",
    "        minus_ends_DFs, minus_cov_DFs = spike_import_singleBp_info(condition_list[i], replicates, \"minus\")\n",
    "        minus_ends.append(minus_ends_DFs)\n",
    "        minus_cov.append(minus_cov_DFs)\n",
    "        \n",
    "        plus_ends_DFs, plus_cov_DFs = spike_import_singleBp_info(condition_list[i], replicates, \"plus\")\n",
    "        plus_ends.append(plus_ends_DFs)\n",
    "        plus_cov.append(plus_cov_DFs)\n",
    "        \n",
    "    # Initialize union dataframe\n",
    "    union_DF = pd.DataFrame(data = all_coords_list, columns = ['coordID'])\n",
    "    \n",
    "    # Initialize lists, where each sub-list inside corresponds to a replicate\n",
    "    # These will each become columns that populate union_DF to enable further calculations\n",
    "    counts_ends_allSamples_allReps = []\n",
    "    counts_cov_allSamples_allReps = []\n",
    "\n",
    "    for i in range(len(condition_list)):\n",
    "        \n",
    "        counts_ends_allSamples_allReps.append([])\n",
    "        counts_cov_allSamples_allReps.append([])\n",
    "        \n",
    "    # For each sample:\n",
    "    for i in range(len(condition_list)):\n",
    "\n",
    "        # For each replicate:\n",
    "        for j in range(replicates):\n",
    "            \n",
    "            print(f'Iterating through {condition_list[i]} replicate {j+1} to extract counts of ends and total coverage')\n",
    "            # Initialize dictionaries of coordID: count pairs\n",
    "            counts_ends_currentRep = {}\n",
    "            counts_cov_currentRep = {}\n",
    "\n",
    "            # Iterate through each coordinate\n",
    "            for k in range(len(all_coords_list)):\n",
    "            \n",
    "                coordID = all_coords_list[k]\n",
    "                strand = coordID[-1]\n",
    "                coordinate = coordID[:-1]\n",
    "            \n",
    "                # For all - strand coordinates:\n",
    "                if strand == \"-\":\n",
    "                    # Get the counts for both ends only and total coverage\n",
    "                    ends_value = int(minus_ends[i][j].loc[minus_ends[i][j]['coordinate'] == int(coordinate),'counts'])\n",
    "                    cov_value = int(minus_cov[i][j].loc[minus_cov[i][j]['coordinate'] == int(coordinate),'counts'])\n",
    "\n",
    "                    # Add key, value pair of coordID, counts to replicate dictionary\n",
    "                    counts_ends_currentRep[coordID] = ends_value\n",
    "                    counts_cov_currentRep[coordID] = cov_value\n",
    "                \n",
    "                # For all + strand coordinates:    \n",
    "                else:\n",
    "                    # Get the counts for both ends only and total coverage\n",
    "                    ends_value = int(plus_ends[i][j].loc[plus_ends[i][j]['coordinate'] == int(coordinate),'counts'])\n",
    "                    cov_value = int(plus_cov[i][j].loc[plus_cov[i][j]['coordinate'] == int(coordinate),'counts'])\n",
    "\n",
    "                    # Add key, value pair of coordID, counts to replicate dictionary\n",
    "                    counts_ends_currentRep[coordID] = ends_value\n",
    "                    counts_cov_currentRep[coordID] = cov_value\n",
    "            \n",
    "            # Add each replicate dictionary to a list of all replicate dictionaries\n",
    "            counts_ends_allSamples_allReps[i].append(counts_ends_currentRep)\n",
    "            counts_cov_allSamples_allReps[i].append(counts_cov_currentRep)        \n",
    "    \n",
    "    print(\"Populating union DF with both end and total coverage counts from each replicate\")\n",
    "    # Add columns to Union_DF\n",
    "    # For each sample:\n",
    "    for i in range(len(counts_ends_allSamples_allReps)):\n",
    "        \n",
    "        # For each replicate:\n",
    "        for j in range(replicates):\n",
    "            column_prefix = f'{condition_list[i]}{j+1}'\n",
    "            union_DF[f'{column_prefix}_ends'] = union_DF['coordID'].map(counts_ends_allSamples_allReps[i][j])\n",
    "            union_DF[f'{column_prefix}_cov'] = union_DF['coordID'].map(counts_cov_allSamples_allReps[i][j])\n",
    "            union_DF[f'{column_prefix}_ER'] = union_DF[f'{column_prefix}_ends'] / union_DF[f'{column_prefix}_cov']\n",
    "\n",
    "    # Add in new columns\n",
    "    for i in range(len(condition_list)):\n",
    "        \n",
    "        union_DF[f'{condition_list[i]}_called'] = \"No\"\n",
    "    \n",
    "    print(\"Annotating conditions in which coordinate was called\")\n",
    "    # Add in whether or not a coordinate was called in a given condition\n",
    "    # Iterate through each coordinate\n",
    "    for i in range(len(union_DF.index)):\n",
    "        \n",
    "        coordID = union_DF['coordID'][i]\n",
    "    \n",
    "        # For each condition:\n",
    "        for j in range(len(condition_list)):\n",
    "            \n",
    "            # Check if the coordID is in the corresponding set of coordIDs for the condition:\n",
    "            if coordID in set(coord_IDs[j]):\n",
    "                union_DF[f'{condition_list[j]}_called'][i] = 'Yes'\n",
    "    \n",
    "    condition_string = \"_\".join(condition_list)\n",
    "    \n",
    "    union_DF['coordID'] = \"Eco_\" + union_DF['coordID']\n",
    "    if multifactor == \"Yes\":\n",
    "        union_DF.to_csv(f'{spike_union_dir}/union_DF_multifactor_2counts.csv')\n",
    "    else:\n",
    "        union_DF.to_csv(f'{spike_union_dir}/union_DF_{condition_string}_2counts.csv')\n",
    "    print(' ')\n",
    "    return union_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "232ca3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DESeq2_design_matrix(name_array, replicates, test):\n",
    "    \"\"\"\n",
    "    test = must be 'ends' or 'readthru'\n",
    "    \"\"\"\n",
    "    if test == 'ends':\n",
    "        conditions = np.repeat(name_array, replicates, axis=0)\n",
    "        design_matrix = pd.DataFrame(data = conditions,\n",
    "                                    columns = ['condition'])\n",
    "        design_matrix['replicate'] = np.tile(np.arange(1,replicates+1),len(name_array))\n",
    "        design_matrix['count_type'] = 'ends'\n",
    "\n",
    "        name_string = '_'.join(name_array)\n",
    "        design_matrix.to_csv(f'{DESeq2_dir}/{name_string}_Wald_design_matrix.csv')\n",
    "        return design_matrix\n",
    "    \n",
    "    elif test == 'readthru':\n",
    "        conditions = np.repeat(name_array, replicates, axis=0)\n",
    "        design_matrix = pd.DataFrame(data = conditions,\n",
    "                                    columns = ['condition'])\n",
    "        design_matrix['replicate'] = np.tile(np.arange(1,replicates+1),len(name_array))\n",
    "        design_matrix['count_type'] = 'ends'\n",
    "\n",
    "        name_string = '_'.join(name_array)\n",
    "        design_matrix.to_csv(f'{DESeq2_dir}/{name_string}_Wald_design_matrix.csv')\n",
    "        return design_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a40c2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DESeq2_countdata_readthru(design_matrix, name_array, union_DF, count_cutoff, multifactor):\n",
    "    \n",
    "    ends_only = union_DF.filter(like = '_ends')\n",
    "    cov_only = union_DF.filter(like = 'cov')\n",
    "    \n",
    "    ends_only.columns = ends_only.columns.str.replace('_ends', '')\n",
    "    cov_only.columns = cov_only.columns.str.replace('_cov', '')\n",
    "\n",
    "    readthru = cov_only.subtract(ends_only)\n",
    "    \n",
    "    ref_sample = name_array[0]\n",
    "    ref_sample_columns = [x for x in readthru.columns if ref_sample in x]\n",
    "    other_columns = [x for x in readthru.columns if not ref_sample in x]\n",
    "    sorted_DF = readthru[ref_sample_columns + other_columns]\n",
    "    indexed_DF = sorted_DF.set_index(union_DF['coordID'])\n",
    "    \n",
    "    design_matrix['sample_name'] = '^' + design_matrix['condition'] + design_matrix['replicate'].astype(str)\n",
    "\n",
    "    DF_list = []\n",
    "    for i in range(len(design_matrix['sample_name'])):\n",
    "        sample_name = design_matrix['sample_name'][i]\n",
    "        condition_subset_DF = indexed_DF.filter(regex = f'{sample_name}')\n",
    "        DF_list.append(condition_subset_DF)\n",
    "        \n",
    "    final_DF = pd.concat(DF_list, axis = 1)\n",
    "    \n",
    "    name_string = '_'.join(name_array)\n",
    "    final_DF.to_csv(f'{DESeq2_dir}/{name_string}_countdata_readthru_{count_cutoff}counts.csv')\n",
    "    if multifactor == \"Yes\":\n",
    "        final_DF.to_csv(f'{DESeq2_dir}/multifactor_countdata_readthru_{count_cutoff}counts.csv')\n",
    "    \n",
    "    return final_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "500106af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DESeq2_countdata_ends(design_matrix, name_array, union_DF, count_cutoff, multifactor):\n",
    "    \n",
    "    ends_only = union_DF.filter(like = '_ends')\n",
    "    ref_sample = name_array[0]\n",
    "    ref_sample_columns = [x for x in ends_only.columns if ref_sample in x]\n",
    "    other_columns = [x for x in ends_only.columns if not ref_sample in x]\n",
    "    sorted_DF = ends_only[ref_sample_columns + other_columns]\n",
    "    indexed_DF = sorted_DF.set_index(union_DF['coordID'])\n",
    "    \n",
    "    name_string = '_'.join(name_array)\n",
    "    indexed_DF.to_csv(f'{DESeq2_dir}/{name_string}_countdata_ends_{count_cutoff}counts.csv')\n",
    "    if multifactor == \"Yes\":\n",
    "        indexed_DF.to_csv(f'{DESeq2_dir}/multifactor_countdata_ends_{count_cutoff}counts.csv')\n",
    "    \n",
    "    return indexed_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04b95f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flexible_DESeq2(condition_subset, test, design, unionDF, count_cutoff):\n",
    "    \"\"\"\n",
    "    Run DESeq2 on input counts.\n",
    "    condition_subset = a list of conditions to compare. \n",
    "        For pairwise, reference sample should come first (denominator in log2 fold change calculations).\n",
    "    test = must be 'ends' or 'readthru'\n",
    "    design = must be 'pairwise' or 'multifactor'\n",
    "    \"\"\"\n",
    "    condition_string = '_'.join(condition_subset)\n",
    "    design_matrix = DESeq2_design_matrix(condition_subset, replicates, test)\n",
    "    \n",
    "    if test == 'ends' and design == 'pairwise':\n",
    "        DESeq2_countdata_ends(design_matrix, condition_subset, unionDF, count_cutoff, \"No\")\n",
    "        \n",
    "        design_formula = '~ condition'\n",
    "        \n",
    "        DESeq2_command = f'Rscript --vanilla Rscripts/CFG_DESeq2_pairwise.R ' + \\\n",
    "            f'-t \"Wald\" ' + \\\n",
    "            f'-r \"{condition_subset[0]}\" ' + \\\n",
    "            f'-n \"{condition_subset[1]}\" ' + \\\n",
    "            f'-d {DESeq2_dir}/{condition_string}_Wald_design_matrix.csv ' + \\\n",
    "            f'-c {DESeq2_dir}/{condition_string}_countdata_ends_{count_cutoff}counts.csv ' + \\\n",
    "            f'-f \"{design_formula}\" ' + \\\n",
    "            f'-s \"{bigWig_dir}/{condition_string}_sizeFactors_ends.csv\" ' + \\\n",
    "            f'-l {DESeq2_dir}/{condition_string}_{count_cutoff}counts_ends_results_Wald_local.csv ' + \\\n",
    "            f'-p {DESeq2_dir}/{condition_string}_{count_cutoff}counts_ends_results_Wald_parametric.csv ' + \\\n",
    "            f'-m {DESeq2_dir}/{condition_string}_{count_cutoff}counts_ends_results_Wald_mean.csv'\n",
    "        quickshell(DESeq2_command, print_output = True)\n",
    "        DESeq2_results_local = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count_cutoff}counts_readthru_results_Wald_local.csv')\n",
    "        DESeq2_results_parametric = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count_cutoff}counts_readthru_results_Wald_parametric.csv')        \n",
    "        DESeq2_results_mean = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count_cutoff}counts_readthru_results_Wald_mean.csv')\n",
    "        min_residuals = min(float(DESeq2_results_local['median_residuals'][0]),\n",
    "                            float(DESeq2_results_parametric['median_residuals'][0]),\n",
    "                            float(DESeq2_results_mean['median_residuals'][0]))\n",
    "        if float(DESeq2_results_local['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_local\n",
    "        elif float(DESeq2_results_parametric['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_parametric\n",
    "        elif float(DESeq2_results_mean['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_mean\n",
    "    \n",
    "    elif test == 'readthru' and design == 'pairwise':\n",
    "        DESeq2_countdata_readthru(design_matrix, condition_subset, unionDF, count_cutoff, \"No\")\n",
    "        \n",
    "        design_formula = '~ condition'\n",
    "        \n",
    "        DESeq2_command = f'Rscript --vanilla Rscripts/CFG_DESeq2_pairwise.R ' + \\\n",
    "            f'-t \"Wald\" ' + \\\n",
    "            f'-r \"{condition_subset[0]}\" ' + \\\n",
    "            f'-n \"{condition_subset[1]}\" ' + \\\n",
    "            f'-d {DESeq2_dir}/{condition_string}_Wald_design_matrix.csv ' + \\\n",
    "            f'-c {DESeq2_dir}/{condition_string}_countdata_readthru_{count_cutoff}counts.csv ' + \\\n",
    "            f'-f \"{design_formula}\" ' + \\\n",
    "            f'-s \"{bigWig_dir}/{condition_string}_sizeFactors_readthru.csv\" ' + \\\n",
    "            f'-l {DESeq2_dir}/{condition_string}_{count_cutoff}counts_readthru_results_Wald_local.csv ' + \\\n",
    "            f'-p {DESeq2_dir}/{condition_string}_{count_cutoff}counts_readthru_results_Wald_parametric.csv ' + \\\n",
    "            f'-m {DESeq2_dir}/{condition_string}_{count_cutoff}counts_readthru_results_Wald_mean.csv'\n",
    "        quickshell(DESeq2_command, print_output = True)\n",
    "        DESeq2_results_local = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count_cutoff}counts_readthru_results_Wald_local.csv')\n",
    "        DESeq2_results_parametric = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count_cutoff}counts_readthru_results_Wald_parametric.csv')        \n",
    "        DESeq2_results_mean = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count_cutoff}counts_readthru_results_Wald_mean.csv')\n",
    "        min_residuals = min(float(DESeq2_results_local['median_residuals'][0]),\n",
    "                            float(DESeq2_results_parametric['median_residuals'][0]),\n",
    "                            float(DESeq2_results_mean['median_residuals'][0]))\n",
    "        if float(DESeq2_results_local['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_local\n",
    "        elif float(DESeq2_results_parametric['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_parametric\n",
    "        elif float(DESeq2_results_mean['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_mean\n",
    "        \n",
    "    elif test == 'ends' and design == 'multifactor':\n",
    "        design_matrix = DESeq2_design_matrix_interaction(condition_subset, replicates, \"raw_ends_counts\", True)\n",
    "        DESeq2_countdata_ends(design_matrix, condition_subset, unionDF, count_cutoff, \"yes\")\n",
    "                \n",
    "        DESeq2_command = f'Rscript --vanilla Rscripts/CFG_DESeq2_multifactor.R ' + \\\n",
    "            f'-d {DESeq2_dir}/multifactor_LRT_int_design_matrix.csv ' + \\\n",
    "            f'-c {DESeq2_dir}/multifactor_countdata_ends_{count_cutoff}counts.csv ' + \\\n",
    "            f'-s \"{bigWig_dir}/multifactor_sizeFactors_ends.csv\" ' + \\\n",
    "            f'-l {DESeq2_dir}/multifactor_{count_cutoff}counts_ends_results_local.csv ' + \\\n",
    "            f'-p {DESeq2_dir}/multifactor_{count_cutoff}counts_ends_results_parametric.csv ' + \\\n",
    "            f'-m {DESeq2_dir}/multifactor_{count_cutoff}counts_ends_results_mean.csv'\n",
    "        quickshell(DESeq2_command, print_output = True)\n",
    "        DESeq2_results_local = pd.read_csv(f'{DESeq2_dir}/multifactor_{count_cutoff}counts_ends_results_local.csv')\n",
    "        DESeq2_results_parametric = pd.read_csv(f'{DESeq2_dir}/multifactor_{count_cutoff}counts_ends_results_parametric.csv')        \n",
    "        DESeq2_results_mean = pd.read_csv(f'{DESeq2_dir}/multifactor_{count_cutoff}counts_ends_results_mean.csv')\n",
    "        min_residuals = min(float(DESeq2_results_local['median_residuals'][0]),\n",
    "                            float(DESeq2_results_parametric['median_residuals'][0]),\n",
    "                            float(DESeq2_results_mean['median_residuals'][0]))\n",
    "        if float(DESeq2_results_local['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_local\n",
    "        elif float(DESeq2_results_parametric['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_parametric\n",
    "        elif float(DESeq2_results_mean['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_mean\n",
    "\n",
    "    elif test == 'readthru' and design == 'multifactor':\n",
    "        design_matrix = DESeq2_design_matrix_interaction(condition_subset, replicates, \"raw_ends_counts\", True)\n",
    "        DESeq2_countdata_readthru(design_matrix, condition_subset, unionDF, count_cutoff, \"Yes\")\n",
    "                \n",
    "        DESeq2_command = f'Rscript --vanilla Rscripts/CFG_DESeq2_multifactor.R ' + \\\n",
    "            f'-d {DESeq2_dir}/multifactor_LRT_int_design_matrix.csv ' + \\\n",
    "            f'-c {DESeq2_dir}/multifactor_countdata_readthru_{count_cutoff}counts.csv ' + \\\n",
    "            f'-s \"{bigWig_dir}/multifactor_sizeFactors_readthru.csv\" ' + \\\n",
    "            f'-l {DESeq2_dir}/multifactor_{count_cutoff}counts_readthru_results_local.csv ' + \\\n",
    "            f'-p {DESeq2_dir}/multifactor_{count_cutoff}counts_readthru_results_parametric.csv ' + \\\n",
    "            f'-m {DESeq2_dir}/multifactor_{count_cutoff}counts_readthru_results_mean.csv'\n",
    "        quickshell(DESeq2_command, print_output = True)\n",
    "        DESeq2_results_local = pd.read_csv(f'{DESeq2_dir}/multifactor_{count_cutoff}counts_readthru_results_local.csv')\n",
    "        DESeq2_results_parametric = pd.read_csv(f'{DESeq2_dir}/multifactor_{count_cutoff}counts_readthru_results_parametric.csv')        \n",
    "        DESeq2_results_mean = pd.read_csv(f'{DESeq2_dir}/multifactor_{count_cutoff}counts_readthru_results_mean.csv')\n",
    "        min_residuals = min(float(DESeq2_results_local['median_residuals'][0]),\n",
    "                            float(DESeq2_results_parametric['median_residuals'][0]),\n",
    "                            float(DESeq2_results_mean['median_residuals'][0]))\n",
    "        if float(DESeq2_results_local['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_local\n",
    "        elif float(DESeq2_results_parametric['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_parametric\n",
    "        elif float(DESeq2_results_mean['median_residuals'][0]) == min_residuals:\n",
    "            return DESeq2_results_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7761cf0",
   "metadata": {},
   "source": [
    "# Run script for range of counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9d729ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def motif_recovery_5enrich_pairwise(count_range, outer_up, outer_down,\n",
    "                                    QCmotifUp, QCmotifDown, QCmotifName,\n",
    "                                    DEmotifUp, DEmotifDown, DEmotifName):\n",
    "    \"\"\"\n",
    "    count_range: which post-nonparametric resampling count thresholds to test for motif recovery or\n",
    "        differentially-expressed TSSs.\n",
    "    outer_up: how much flanking sequence to extract upstream of the TSS coordinate (positive number).\n",
    "    outer_down: how much flanking sequence to extract downstream of the TSS coordinate (positive number).\n",
    "    QCmotifUp: how far upstream of the TSS to scan for the quality-control motif.\n",
    "    QCmotifDown: how far downstream of the TSS to scan for the quality-control motif.\n",
    "    QCmotifName: name of quality-control motif (for file names).\n",
    "    DEmotifUp: how far upstream of the TSS to scan for the differentially-expressed motif.\n",
    "        i.e. motif associated with differentially-expressed genes in response to a TF.\n",
    "    DEmotifDown: how far downstream of the TSS to scan for the differentially-expressed motif.\n",
    "    DEmotifName: name of differentially-expressed motif (for file names).\n",
    "    \"\"\"\n",
    "    # For each end count cutoff:\n",
    "    for count_cutoff in count_range:\n",
    "        \n",
    "        print(\"count_cutoff: \" + str(count_cutoff))\n",
    "        MEME_dir_counts = f'{MEME_dir}/{condition_string}_{count_cutoff}counts'\n",
    "        quickshell(f'mkdir {MEME_dir_counts}')\n",
    "        \n",
    "        # Generate consensus peaks for blacklist DF\n",
    "        # (consensus = present in all 3 replicates and all of them meet the count cutoff)\n",
    "        if blacklist_sample != 'gDNA':\n",
    "            blacklist_DF = consensus_peaks(blacklist_sample, replicates, count_cutoff)\n",
    "        if blacklist_sample == 'gDNA':\n",
    "            blacklist_DF = consensus_peaks_gDNA(replicates, count_cutoff)\n",
    "                \n",
    "        # For all conditions examined\n",
    "        for condition in condition_subset:\n",
    "            \n",
    "            # Generate consensus peaks for experimental samples\n",
    "            preBL = consensus_peaks(condition, replicates, count_cutoff)\n",
    "            # Filter out coordinates found in the blacklist sample (e.g. gDNA, core)\n",
    "            postBL = remove_blacklist_coordinates(preBL, blacklist_DF, 'end')\n",
    "            postBL.to_csv(f'{postBL_dir}/' + \\\n",
    "                                    f'postBlackList_{condition}_{count_cutoff}counts.csv')\n",
    "            \n",
    "            postBL = pd.read_csv(f'{postBL_dir}/' + \\\n",
    "                                    f'postBlackList_{condition}_{count_cutoff}counts.csv')\n",
    "            \n",
    "            # Prepare for sequence extraction\n",
    "            # Since each replicate gets its own row, only take replicate 1 for seq extraction\n",
    "            seqExtractDF = postBL.loc[postBL['replicate'] == 1,]\n",
    "            seqExtractDF['coordinate'] = seqExtractDF['end']\n",
    "            # Generate DF with random coordinates as control sequences\n",
    "            random_coordinates = np.random.randint(min(seqExtractDF['coordinate']),\n",
    "                                                   max(seqExtractDF['coordinate']),\n",
    "                                                   size = len(seqExtractDF.index))\n",
    "                                                      \n",
    "            control_coord_dict = {'coordinate':list(random_coordinates), 'strand':list(seqExtractDF['strand'])}\n",
    "            control_coord_DF = pd.DataFrame(data = control_coord_dict)\n",
    "           \n",
    "            meme_validation(seqExtractDF, control_coord_DF, outer_up, outer_down,\n",
    "                            coordMotifUp, coordMotifDown, condition, count_cutoff,\n",
    "                            'xstreme', coordMotifName, MEME_dir_counts)\n",
    "\n",
    "                                        \n",
    "        # Generate union of called coordinates\n",
    "        unionDF = union_DF(condition_subset, count_cutoff)\n",
    "        unionDF = pd.read_csv(f'{union_dir}/union_DF_{condition_string}_{count_cutoff}counts.csv')\n",
    "       \n",
    "        # Separate coordinate and strand for sequence extraction\n",
    "        coordinate_str = unionDF['coordID'].str[:-1]\n",
    "        unionDF['coordinate'] = coordinate_str.astype(int) - 2\n",
    "        unionDF['strand'] = unionDF['coordID'].str[-1:]\n",
    "              \n",
    "        # Generate DF with random coordinates as control sequences\n",
    "        random_coordinates = np.random.randint(min(unionDF['coordinate']),\n",
    "                                                max(unionDF['coordinate']),\n",
    "                                                size = len(unionDF.index))\n",
    "                                                       \n",
    "        control_coord_dict = {'coordinate': list(random_coordinates), 'strand': list(unionDF['strand'])}\n",
    "        control_coord_DF = pd.DataFrame(data = control_coord_dict)\n",
    "        \n",
    "        meme_validation(unionDF, control_coord_DF, outer_up, outer_down,\n",
    "                        coordMotifUp, coordMotifDown, condition_string, count_cutoff,\n",
    "                        'xstreme', coordMotifName, MEME_dir_counts)\n",
    "\n",
    "        \n",
    "        # Generate union of called coordinates (spike)\n",
    "        spikeUnionDF = spike_union_DF(condition_subset, 2)\n",
    "        spikeUnionDF = pd.read_csv(f'{spike_union_dir}/union_DF_{condition_string}_2counts.csv')\n",
    "        \n",
    "        coordinate_str = spikeUnionDF['coordID'].str[4:-1]\n",
    "        spikeUnionDF['coordinate'] = coordinate_str.astype(int) - 2\n",
    "        spikeUnionDF['strand'] = spikeUnionDF['coordID'].str[-1:]\n",
    "        \n",
    "        # Add spike coords to experimental coords\n",
    "        union_addSpike = pd.concat([unionDF, spikeUnionDF], axis = 0)\n",
    "        \n",
    "        # Run DESeq2 on read-thru to filter out coords\n",
    "        DESeq2_readthru = flexible_DESeq2(condition_subset, 'readthru', 'pairwise', union_addSpike, count_cutoff)\n",
    "        DESeq2_readthru_sig = DESeq2_readthru.loc[DESeq2_readthru['padj'] <= 0.05,]\n",
    "        readthru_filtered_DF = remove_blacklist_coordinates(union_addSpike, DESeq2_readthru_sig, 'coordID')\n",
    "        \n",
    "        # Actual differential expression analysis:\n",
    "        # Run DESeq2\n",
    "        DESeq2_results = flexible_DESeq2(condition_subset, 'ends', 'pairwise', readthru_filtered_DF, count_cutoff)\n",
    "        \n",
    "        padj_005 = DESeq2_results.loc[DESeq2_results['padj'] < 0.05,]\n",
    "            \n",
    "        # Separate coordinate and strand for sequence extraction\n",
    "        coordinate_str = padj_005['coordID'].str[:-1]\n",
    "        padj_005['coordinate'] = coordinate_str.astype(int) - 2\n",
    "        padj_005['strand'] = padj_005['coordID'].str[-1:]\n",
    "        \n",
    "        readthru_DF = readthru_filtered_DF[~readthru_filtered_DF['coordID'].str.contains(\"Eco\")]\n",
    "        coordinate_str = readthru_DF['coordID'].str[:-1]\n",
    "        readthru_DF['coordinate'] = coordinate_str.astype(int) - 2\n",
    "        readthru_DF['strand'] = readthru_DF['coordID'].str[-1:]\n",
    "        \n",
    "        # Search in general for motif in entire promoter region\n",
    "        meme_validation(padj_005, readthru_DF, outer_up, outer_down,\n",
    "                        TFmotifUp, TFmotifDown, condition_string, count_cutoff,\n",
    "                        'xstreme', TFmotifName, MEME_dir_counts)\n",
    "            \n",
    "         # Activation region\n",
    "        activated_padj005 = padj_005.loc[padj_005['log2FoldChange'] > 0,]\n",
    "        meme_validation(activated_padj005, readthru_filtered_DF, outer_up, outer_down, TFmotifUp, TFmotifDown,\n",
    "                         condition_string, count_cutoff, 'xstreme', f'{TFmotifName}_activated', MEME_dir_counts)\n",
    "        \n",
    "         # Inhibition region\n",
    "        inhibited_padj005 = padj_005.loc[padj_005['log2FoldChange'] < 0,]\n",
    "        meme_validation(inhibited_padj005, readthru_filtered_DF, outer_up, outer_down, TFmotifUp, TFmotifDown,\n",
    "                         condition_string, count_cutoff, 'xstreme', f'{TFmotifName}_inhibited', MEME_dir_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cc5f1c8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "motif_recovery_5enrich_pairwise(count_cutoff_range, outer_up, outer_down,\n",
    "                       QCmotifUp, QCmotifDown, QCmotifName,\n",
    "                       DEmotifUp, DEmotifDown, DEmotifName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b5a11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multifactor_count_thresholds(count_range, enrich_type):\n",
    "\n",
    "    # For each end count cutoff:\n",
    "    for count_cutoff in count_range:\n",
    "                \n",
    "        # Generate consensus peaks for blacklist DF\n",
    "        # (consensus = present in all 3 replicates and all of them meet the count cutoff)\n",
    "        if enrich_type == \"5end\":\n",
    "            blacklist_DF = consensus_peaks(blacklist_sample, replicates, count_cutoff)\n",
    "            blacklist_DF = blacklist_DF.loc[blacklist_DF['end'] == 'a',]\n",
    "        elif enrich_type == \"3end\":\n",
    "            blacklist_DF = consensus_peaks_gDNA(replicates, count_cutoff)\n",
    "        \n",
    "        # For all conditions examined (for optimization of motif recovery: +/- CRP):\n",
    "        for condition in condition_subset:\n",
    "            \n",
    "            # Generate consensus peaks for experimental samples\n",
    "            preBL = consensus_peaks(condition, replicates, count_cutoff)\n",
    "            # Filter out coordinates found in the noise/artifact sample (e.g. gDNA, core)\n",
    "            postBL = remove_blacklist_coordinates(preBL, blacklist_DF, 'end')\n",
    "            postBL.to_csv(f'{postBL_dir}/' + \\\n",
    "                                    f'postBlackList_{condition}_{count_cutoff}counts.csv')\n",
    "            \n",
    "            postBL = pd.read_csv(f'{postBL_dir}/' + \\\n",
    "                                    f'postBlackList_{condition}_{count_cutoff}counts.csv')\n",
    "                                        \n",
    "        # Generate union of called coordinates\n",
    "        unionDF = union_DF(condition_subset, count_cutoff, \"Yes\")\n",
    "\n",
    "        # Separate coordinate and strand for sequence extraction\n",
    "        coordinate_str = unionDF['coordID'].str[:-1]\n",
    "        unionDF['coordinate'] = coordinate_str.astype(int) - 2\n",
    "        unionDF['strand'] = unionDF['coordID'].str[-1:]\n",
    "        \n",
    "        # Generate union of called coordinates (spike)\n",
    "        spikeUnionDF = spike_union_DF(condition_subset, 2, \"Yes\")\n",
    "        coordinate_str = spikeUnionDF['coordID'].str[4:-1]\n",
    "        spikeUnionDF['coordinate'] = coordinate_str.astype(int) - 2\n",
    "        spikeUnionDF['strand'] = spikeUnionDF['coordID'].str[-1:]\n",
    "        \n",
    "        # Add spike coords to normal coords\n",
    "        union_addSpike = pd.concat([unionDF, spikeUnionDF], axis = 0)\n",
    "        \n",
    "        # Run DESeq2 on read-thru to filter out coords\n",
    "        DESeq2_readthru = flexible_DESeq2(condition_subset, 'readthru', 'multifactor',\n",
    "                                          union_addSpike, count_cutoff)\n",
    "        DESeq2_readthru_sig = DESeq2_readthru.loc[DESeq2_readthru['padj'] <= 0.05,]\n",
    "        readthru_filtered_DF = remove_blacklist_coordinates(union_addSpike, DESeq2_readthru_sig, 'coordID')\n",
    "\n",
    "        # Actual differential expression analysis:\n",
    "        # Run DESeq2\n",
    "        DESeq2_results = flexible_DESeq2(condition_subset, 'ends', 'multifactor',\n",
    "                                         readthru_filtered_DF, count_cutoff)\n",
    "        \n",
    "        padj_005 = DESeq2_results.loc[DESeq2_results['padj'] < 0.05,]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3dcae1",
   "metadata": {},
   "source": [
    "# Compile stats in table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "833abaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_results(count_range, QCmotifName, QCmotifRegex,\n",
    "                   DESeq2_sig_cutoff, DEmotifName, DEmotifRegex):\n",
    "\n",
    "    \n",
    "    condition_string = \"_\".join(condition_subset)\n",
    "    \n",
    "    # Initialize DF\n",
    "    stats_DF = pd.DataFrame(data = count_range, columns = ['count_cutoff'])\n",
    "    values_for_DF = []\n",
    "    \n",
    "    for count_cutoff in count_range:\n",
    "        # MEME results location\n",
    "        MEME_dir_counts = f'{MEME_dir}/{condition_string}_{count_cutoff}counts'\n",
    "        \n",
    "        sample_dict = {}\n",
    "        \n",
    "        sample_dict['count_cutoff'] = count_cutoff\n",
    "        \n",
    "        for condition in condition_subset:\n",
    "            \n",
    "            preBL = pd.read_csv(f'{preBL_dir}/' + \\\n",
    "                    f'preBlackList_{condition}_consensus_calls_{replicates}reps_{count_cutoff}counts.csv')\n",
    "            sample_dict[f'{condition}_preBL_numCoords'] = (len(preBL.index)/replicates)\n",
    "            \n",
    "            postBL = pd.read_csv(f'{postBL_dir}/' + \\\n",
    "                                    f'postBlackList_{condition}_{count_cutoff}counts.csv')\n",
    "            sample_dict[f'{condition}_postBL_numCoords'] = (len(postBL.index)/replicates)\n",
    "            \n",
    "            xstreme_results = pd.read_table(f'{MEME_dir_counts}/' + \\\n",
    "                                            f'{condition}_{count_cutoff}counts_{QCmotifName}_xstreme/' + \\\n",
    "                                            f'xstreme.tsv')\n",
    "            consensus = xstreme_results['CONSENSUS'][0]\n",
    "            sample_dict[f'{condition}_top_motif'] = consensus\n",
    "            match_motif = re.search(QCmotifRegex, consensus)\n",
    "            if match_motif == None:\n",
    "                sample_dict[f'{QCmotifName}?'] = False\n",
    "            else:\n",
    "                sample_dict[f'{QCmotifName}?'] = True\n",
    "            \n",
    "            try:\n",
    "                sea_results = pd.read_table(f'{MEME_dir_counts}/' + \\\n",
    "                                            f'{condition}_{count_cutoff}counts_{QCmotifName}_xstreme/' + \\\n",
    "                                            f'sea_out/sea.tsv')\n",
    "            \n",
    "                sample_dict[f'{condition}_SEA_LogPvalue'] = sea_results['LOG_PVALUE'][0]\n",
    "                sample_dict[f'{condition}_SEA_Sig?'] = sea_results['LOG_PVALUE'][0] < sea_results['LOG_EVALUE'][0]\n",
    "                sample_dict[f'{condition}_TP'] = sea_results['TP'][0]\n",
    "                sample_dict[f'{condition}_TP%'] = sea_results['TP%'][0]\n",
    "                sample_dict[f'{condition}_SEA_EnrichRatio'] = sea_results['ENR_RATIO'][0]\n",
    "            \n",
    "                fimo_results = pd.read_table(f'{MEME_dir_counts}/' + \\\n",
    "                                            f'{condition}_{count_cutoff}counts_{QCmotifName}_xstreme/' + \\\n",
    "                                            f'fimo_out_1/fimo.tsv')   \n",
    "            \n",
    "                sample_dict[f'{condition}_FIMO_match'] = len(fimo_results.dropna().index)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        union_DF = pd.read_csv(f'{union_dir}/union_DF_{condition_string}_{count_cutoff}counts.csv')\n",
    "        sample_dict[f'union_numCoords_preBL'] = len(union_DF.index)\n",
    "        \n",
    "        DESeq2_results_local = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count_cutoff}counts_{DESeq_test}_' + \\\n",
    "                                        f'results_Wald_local.csv')\n",
    "        DESeq2_results_parametric = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count_cutoff}counts_{DESeq_test}_' + \\\n",
    "                                        f'results_Wald_parametric.csv')\n",
    "        DESeq2_results_mean = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count_cutoff}counts_{DESeq_test}_' + \\\n",
    "                                        f'results_Wald_mean.csv')\n",
    "        min_residuals = min(float(DESeq2_results_local['median_residuals'][0]),\n",
    "                            float(DESeq2_results_parametric['median_residuals'][0]),\n",
    "                            float(DESeq2_results_mean['median_residuals'][0]))\n",
    "        if float(DESeq2_results_local['median_residuals'][0]) == min_residuals:\n",
    "                DESeq_results = DESeq2_results_local\n",
    "        elif float(DESeq2_results_parametric['median_residuals'][0]) == min_residuals:\n",
    "                DESeq_results = DESeq2_results_parametric\n",
    "        elif float(DESeq2_results_mean['median_residuals'][0]) == min_residuals:\n",
    "                DESeq_results = DESeq2_results_mean\n",
    "            \n",
    "        sample_dict['DESeq_fitType'] = DESeq_results['fitType'][0]\n",
    "        sample_dict['DESeq_medianAbsResiduals'] = DESeq_results['median_residuals'][0]\n",
    "        \n",
    "        sample_dict[f'union_numCoords_postBL'] = len(DESeq_results.index)\n",
    "        sig_DESeq = DESeq_results.loc[DESeq_results['padj'] <= padj_cutoff,]\n",
    "        sample_dict[f'num_padj005'] = len(sig_DESeq.index)\n",
    "        activated = sig_DESeq.loc[sig_DESeq['log2FoldChange'] > 0,]\n",
    "        sample_dict[f'num_activated'] = len(activated.index)\n",
    "        inhibited = sig_DESeq.loc[sig_DESeq['log2FoldChange'] < 0,]\n",
    "        sample_dict[f'num_inhibited'] = len(inhibited.index)\n",
    "        \n",
    "        xstreme_results = pd.read_table(f'{MEME_dir_counts}/' + \\\n",
    "                                        f'{condition_string}_{count_cutoff}counts_{DEmotifName}_xstreme/' + \\\n",
    "                                        f'xstreme.tsv')\n",
    "        consensus = xstreme_results['CONSENSUS'][0]\n",
    "\n",
    "        sample_dict[f'top_motif'] = consensus\n",
    "\n",
    "        try:\n",
    "            match_motif = re.search(DE_motif_regex, consensus)            \n",
    "            \n",
    "            if match_motif == None:\n",
    "                sample_dict[f'{DEmotifName}?'] = False\n",
    "            else:\n",
    "                sample_dict[f'{DEmotifName}?'] = True\n",
    "        \n",
    "            xstreme_results = pd.read_table(f'{MEME_dir_counts}/' + \\\n",
    "                                           f'{condition_string}_{count_cutoff}counts_{DEmotifName}_xstreme/' + \\\n",
    "                                           f'xstreme.tsv')\n",
    "        \n",
    "            sample_dict['xstreme_Evalue'] = xstreme_results['EVALUE'][0]\n",
    "        \n",
    "            try:\n",
    "                sea_results = pd.read_table(f'{MEME_dir_counts}/' + \\\n",
    "                                            f'{condition_string}_{count_cutoff}counts_{DEmotifName}_xstreme/' + \\\n",
    "                                            f'sea_out/sea.tsv')\n",
    "            \n",
    "                sample_dict[f'SEA_LogPvalue'] = sea_results['LOG_PVALUE'][0]\n",
    "                sample_dict[f'SEA_Sig?'] = sea_results['LOG_PVALUE'][0] < sea_results['LOG_EVALUE'][0]\n",
    "                sample_dict[f'TP'] = sea_results['TP'][0]\n",
    "                sample_dict[f'TP%'] = sea_results['TP%'][0]\n",
    "                sample_dict[f'SEA_EnrichRatio'] = sea_results['ENR_RATIO'][0]\n",
    "            \n",
    "                fimo_results = pd.read_table(f'{MEME_dir_counts}/' + \\\n",
    "                                            f'{condition_string}_{count_cutoff}counts_{DEmotifName}_xstreme/' + \\\n",
    "                                            f'fimo_out_1/fimo.tsv')   \n",
    "            \n",
    "                sample_dict[f'FIMO_match'] = len(fimo_results.dropna().index)\n",
    "        \n",
    "                # Find most abundant positions\n",
    "                motif_length = len(fimo_results['motif_id'][0])\n",
    "                fimo_results['center'] = fimo_results['start'] - (TFmotifUp - (motif_length / 2))\n",
    "                freqs = plt.hist(fimo_results['center'])\n",
    "                freqs_include_right = np.append(freqs[0], 0)\n",
    "                freqs_include_both = np.insert(freqs_include_right, 0, 0)\n",
    "                peaks, properties = find_peaks(freqs_include_both)\n",
    "                peaks_norm = peaks - 1\n",
    "                center_peaks = freqs[1][peaks_norm]\n",
    "\n",
    "                for i in range(len(center_peaks)):\n",
    "    \n",
    "                    sample_dict[f'peak{i+1}_center'] = center_peaks[i]\n",
    "                    sample_dict[f'peak{i+1}_height'] = freqs[0][peaks_norm][i]\n",
    "        \n",
    "                values_for_DF.append(sample_dict)\n",
    "            except:\n",
    "                pass\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    final_DF = pd.DataFrame.from_dict(values_for_DF)\n",
    "    final_DF.to_csv(f'{motif_compilation_dir}/{condition_string}_{replicates}reps_' + \\\n",
    "                    f'{DEmotifName}motif_optimization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "605dfbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus = compile_results(count_cutoff_range, coordMotifName, coordMotifRegex,\n",
    "                  DESeq2_sig_cutoff, DEmotifName, DEmotifRegex)\n",
    "\n",
    "consensus_activated = compile_results(count_cutoff_range, QCMotifName, QCMotifRegex,\n",
    "                  DESeq2_sig_cutoff, f'{DEmotifName}_activated', DEmotifRegex)\n",
    "\n",
    "consensus_inhibited = compile_results(count_cutoff_range, QCMotifName, QCMotifRegex,\n",
    "                  DESeq2_sig_cutoff, f'{DEmotifName}_inhibited', DEmotifRegex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c18f986",
   "metadata": {},
   "source": [
    "# Add all motif hits to DESeq2 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba630b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sea_get_matches(motif_dir, MEME_dir_counts, condition_string, count_cutoff, motif_name):\n",
    "    \"\"\"Re-run SEA without the --no-seqs argument hard-coded into xstreme to get the true positives.\"\"\"\n",
    "    sea_call = f'{MEME_path}/sea --verbosity 4 ' + \\\n",
    "                    f'--oc {motif_dir}/{condition_string}_{count_cutoff}counts_{motif_name}_seaSeqs ' + \\\n",
    "                    f'--qvalue --thresh 1 --order 2 --bfile {motif_dir}/background ' + \\\n",
    "                    f'--seed 0 --align center --motif-pseudo 0.01 ' + \\\n",
    "                    f'--m {motif_dir}/meme_out/meme.xml --m {motif_dir}/streme_out/streme.xml ' + \\\n",
    "                    f'--p {MEME_dir_counts}/{condition_string}_{motif_name}_{count_cutoff}counts.fasta ' + \\\n",
    "                    f'--n {MEME_dir_counts}/{condition_string}_{motif_name}_control_{count_cutoff}counts.fasta'\n",
    "    quickshell(sea_call, print_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "155e1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fimo_changeThreshold(motif_dir, MEME_dir_counts, discovery_algorithm, condition_string, count_cutoff, motif,\n",
    "                        motif_name):\n",
    "    \"\"\"Re-run fimo with a lower threshold to extract all true positives from SEA analysis.\"\"\"\n",
    "    fimo_call = f'{MEME_path}/fimo ' + \\\n",
    "                f'--parse-genomic-coord --verbosity 4 --oc {motif_dir}/fimo_out_lowThres ' + \\\n",
    "                f'--bfile {motif_dir}/background --motif {motif} --thresh 5e-2 ' + \\\n",
    "                f'{motif_dir}/{discovery_algorithm}_out/{discovery_algorithm}.xml ' + \\\n",
    "                f'{MEME_dir_counts}/{condition_string}_{motif_name}_{count_cutoff}counts.fasta'\n",
    "    quickshell(fimo_call, print_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478ce31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_DESeq2_motif(motif_name, optimal_count_threshold):\n",
    "    \n",
    "    all_counts = pd.read_csv(f'{motif_compilation_dir}/{condition_string}_{replicates}reps_' + \\\n",
    "                    f'{DEmotifName}motif_optimization.csv')\n",
    "    # Find peaks in true positives (TP)\n",
    "    freqs = np.array(all_counts['TP'])\n",
    "    freqs_include_right = np.append(freqs, 0)\n",
    "    freqs_include_both = np.insert(freqs_include_right, 0, 0)\n",
    "    peaks, properties = find_peaks(freqs_include_both)\n",
    "    peaks_norm = peaks - 1\n",
    "    TP_peaks = freqs[peaks_norm]\n",
    "    \n",
    "    # Here, if you don't provide an optimal count threshold based on manual inspection,\n",
    "    # the optimal count threshold will be the threshold with the highest TP #\n",
    "    max_counts = all_counts.loc[all_counts['TP'] == max(TP_peaks),'count_cutoff']\n",
    "    max_counts_indices = max_counts.index\n",
    "#    optimal_count_threshold = max_counts[max_counts_indices[0]]\n",
    "\n",
    "    all_coords_DF = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{optimal_count_threshold}counts_' + \\\n",
    "                        f'ends_results_Wald_local.csv')\n",
    "\n",
    "    # Annotate whether each coordinate is a hit at other count thresholds\n",
    "    for count in count_cutoff_range:\n",
    "    \n",
    "        count_DF = pd.read_csv(f'{DESeq2_dir}/{condition_string}_{count}counts_' + \\\n",
    "                        f'ends_results_Wald.csv')\n",
    "        count_sig_DF = count_DF.loc[count_DF['padj'] <= sig_cutoff,]\n",
    "        all_coords_DF[f'hit_in_{count}counts'] = all_coords_DF['coordID'].isin(count_sig_DF['coordID']) \n",
    "    \n",
    "    count_cols = all_coords_DF.filter(like = 'hit_in')\n",
    "    all_coords_DF['num_thresholds_met'] = count_cols.sum(axis = 1)\n",
    "    \n",
    "    # Annotate motif information\n",
    "    MEME_count_dir = f'{MEME_outputs_dir}/{condition_string}_{optimal_count_threshold}counts'\n",
    "    TFmotif_dir = f'{MEME_count_dir}/{condition_string}_{optimal_count_threshold}counts_{motif_name}_xstreme'\n",
    "    sea_get_matches(TFmotif_dir, MEME_count_dir, condition_string, optimal_count_threshold, motif_name)\n",
    "    sea_DF = pd.read_table(f'{TFmotif_dir}/{condition_string}_{optimal_count_threshold}counts_{motif_name}_seaSeqs/' + \\\n",
    "                           f'sequences.tsv')\n",
    "    sea_DF_onlyTop = sea_DF.loc[sea_DF['motif_ID'] == sea_DF['motif_ID'][0],]\n",
    "    sea_DF_TP = sea_DF_onlyTop.loc[sea_DF_onlyTop['seq_Class'] == 'tp',]\n",
    "    sea_DF_FP = sea_DF_onlyTop.loc[sea_DF_onlyTop['seq_Class'] == 'fp',]\n",
    "\n",
    "    motif = sea_DF['motif_ID'][0]\n",
    "    if 'MEME' in sea_DF['motif_ALT_ID'][0]:\n",
    "        fimo_changeThreshold(TFmotif_dir, MEME_count_dir, 'meme',\n",
    "                             condition_string, optimal_count_threshold, motif, motif_name)\n",
    "    elif 'STREME' in sea_DF['motif_ALT_ID'][0]:\n",
    "        fimo_changeThreshold(TFmotif_dir, MEME_count_dir, 'streme',\n",
    "                             condition_string, optimal_count_threshold, motif, motif_name)\n",
    "\n",
    "\n",
    "    fimo_DF = pd.read_table(f'{TFmotif_dir}/fimo_out_lowThres/' + \\\n",
    "                           f'fimo.tsv').dropna()\n",
    "    fimo_truePositives = fimo_DF.loc[fimo_DF['sequence_name'].isin(sea_DF_TP['seq_ID']),]\n",
    "    fimo_truePositives['seq_Class'] = 'tp'\n",
    "    fimo_falsePositives = fimo_DF.loc[fimo_DF['sequence_name'].isin(sea_DF_FP['seq_ID']),]\n",
    "    fimo_falsePositives['seq_Class'] = 'fp'\n",
    "    fimo_allPos = pd.concat([fimo_truePositives, fimo_falsePositives], axis = 0)\n",
    "\n",
    "    fimo_allPos['top_ranked?'] = False\n",
    "\n",
    "    for coord_ID in fimo_allPos['sequence_name'].unique():\n",
    "    \n",
    "        sub_DF = fimo_allPos.loc[(fimo_allPos['sequence_name'] == coord_ID),]\n",
    "        sub_DF.loc[sub_DF['score'] == max(sub_DF['score']),'top_ranked?'] = True\n",
    "        fimo_allPos.loc[(fimo_truePositives['sequence_name'] == coord_ID),'top_ranked?'] = sub_DF['top_ranked?']\n",
    "    \n",
    "    fimo_allPositives_topScore = fimo_allPos.loc[fimo_allPos['top_ranked?'] == True,]\n",
    "    fimo_allPositives_topScore['coord'] = fimo_allPositives_topScore['sequence_name'].str[:-1]\n",
    "    fimo_allPositives_topScore['strand'] = fimo_allPositives_topScore['sequence_name'].str[-1:]\n",
    "    fimo_allPositives_topScore['correct_coord'] = fimo_allPositives_topScore['coord'].astype(int) + 2\n",
    "    \n",
    "    fimo_allPositives_topScore['coordID'] = fimo_allPositives_topScore['correct_coord'].astype(str) + fimo_allPositives_topScore['strand']\n",
    "    new_DF = pd.merge(all_coords_DF, fimo_allPositives_topScore, on='coordID', how='left')\n",
    "    motifs = new_DF['motif_id'].dropna()\n",
    "    motif_indices = motifs.index\n",
    "    motif_length = len(motifs[motif_indices[0]])\n",
    "    upstream_end = TFmotifUp\n",
    "    new_DF['motif_center'] = new_DF['start'] - (upstream_end - (motif_length / 2))\n",
    "    colsToKeep = ['coordID','baseMean','log2FoldChange','padj','num_thresholds_met','motif_id',\n",
    "                 'score','p-value','q-value','matched_sequence','motif_center']\n",
    "    filtered_cols = new_DF[new_DF.columns[new_DF.columns.isin(colsToKeep)]]\n",
    "    unique_DF = filtered_cols.drop_duplicates(subset = ['coordID'], keep = 'last')\n",
    "    unique_DF.to_csv(f'{motif_compilation_dir}/' + \\\n",
    "                         f'{condition_string}_{motif_name}_{optimal_count_threshold}counts_DESeq2.csv')   \n",
    "    return unique_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "integrate_DESeq2_motif(motif_name = 'min100plus30',\n",
    "                       optimal_count_threshold = 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
